<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <title>TextSynth Server</title>
   <style>
    
table.sortable, th {
    border: 1px solid black;
    border-collapse: collapse;
}

table.sortable tbody tr:nth-child(odd) {
    background-color: #ddd;
}

table.sortable td {
    border-left: 1px solid black;
    border-right: 1px solid black;
}

td, th {
    padding: 3px;
}

th {
    font-size: 85%;
    font-weight: bold;
    text-align: left;
    padding: 4px;
    margin: 1px;
}

.num {
    text-align: right;
}

/* sorting */

table.sortable th button {
  font-size: 100%;
  font-weight: bold;
  background: transparent;
  border: none;
  display: inline;
  right: 0;
  left: 0;
  top: 0;
  bottom: 0;
  width: 100%;
  text-align: left;
  outline: none;
  cursor: pointer;
}

table.sortable th[aria-sort="descending"] span::after {
  content: "▼";
  color: currentcolor;
  font-size: 100%;
  top: 0;
}

table.sortable th[aria-sort="ascending"] span::after {
  content: "▲";
  color: currentcolor;
  font-size: 100%;
  top: 0;
}

table.sortable th:not([aria-sort]) button:focus span::after,
table.sortable th:not([aria-sort]) button:hover span::after {
  content: "▼";
  color: currentcolor;
  font-size: 100%;
  top: 0;
}
   </style>
   <script src="sortable-table.js"></script>
</head>
<body>
<h1>TextSynth Server</h1>

<h2>News</h2>

<ul>
  <li>2025-03-08: Added Parler-TTS text-to-speech model. Added real
  time WebSocket API for audio transcription and voice chat. Added
  BERT embeddings.</li>
  <li>2024-09-30: Added Llama 3.2, phi3, Qwen2 and gte_qwen2 models
  support. Added embeddings endpoint. Added cpu offloading.</li>
  <li>2024-08-03: Added Llama 3.1 model support.</li>
  <li>2024-05-21: Added Llama 3 model support.</li>
  <li>2024-01-20: Added Mixtral model support. Added fast Whisper
  based speech to text transcription.</li>
  <li>2023-10-21: CUDA support in the Windows version, mistral model
  support. Speculative sampling is supported. BNF grammar and JSON
  schema sampling.</li>
  <li>2023-08-07: The GPU version and model conversion utilities are
  now freely available.</li>
  <li>2023-07-21: The MPT and Llama 2 models are supported.</li>
  <li>2023-06-10: New <b>ts_chat</b> utility to chat with language models. The Falcon and RedPajama-INCITE models are supported.</li>
  <li>2023-03-26: The <a href="https://github.com/facebookresearch/fairseq/tree/nllb">NLLB200</a> and <a href="https://arxiv.org/abs/2205.05131">flan UL2</a> models have been added. An HTML GUI is now available in <b>ts_server</b>.</li>
</ul>

<h2>Introduction</h2>

<b>ts_server</b> is a web server proposing a REST API to large
language models. They can be used for example for text completion,
question answering, classification, chat, translation, image
generation, audio transcription, speech synthesis, ...
<p>It has the following characteristics:
<ul>
  <li>All is included in a single binary. Very few external dependencies (Python is not needed) so installation is easy.</li>
  <li>Supports many Transformer variants (<a href="https://github.com/kingoflolz/mesh-transformer-jax">GPT-J</a>, <a href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a>, <a href="https://github.com/EleutherAI/gpt-neo">GPT-Neo</a>, <a href="https://github.com/facebookresearch/metaseq">OPT</a>, <a href="https://github.com/pytorch/fairseq/tree/main/examples/moe_lm">Fairseq GPT</a>, <a href="https://arxiv.org/abs/2010.11125">M2M100</a>, <a href="https://github.com/salesforce/CodeGen">CodeGen</a>, <a href="https://github.com/openai/gpt-2">GPT2</a>, <a href="https://arxiv.org/abs/2210.11416">T5</a>, <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a>, <a href="https://github.com/facebookresearch/llama">LLAMA</a>, <a href="https://falconllm.tii.ae/">Falcon</a>, <a href="https://github.com/mosaicml/llm-foundry">MPT</a>, Llama 3.2, Mistral, Mixtral, Qwen2, Phi3, Whisper, Parler-TTS) and <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a>.</li>
  <li>Integrated REST JSON API for text completion, translation, image generation, audio transcription and speech synthesis. It is used by <a href="https://textsynth.com/documentation.html">textsynth.com</a>.</li>
  <li>Integrated WebSocket API for real time audio transcription and voice chat (experimental).</li>
  <li>Integrated HTML GUI for testing.</li>
  <li>Very high performance for small and large batches on CPU and GPU. Support of dynamic batching to handle a large number of simultaneous requests.</li>
  <li>Efficient custom 8, 4 and 3 bit quantization. Our quantized models are thoroughly evaluated on several <a href="index.html#models">standard tasks</a> to ensure good performance.</li>
  <li>Larger models work optimally on lower cost GPUs (e.g. RTX 3090, RTX A6000) thanks to efficient quantization.</li>
  <li>Support of speculative sampling for even faster inference.</li>
  <li>Support of grammar based sampling to constraint the model output according to a BNF grammar or a JSON schema.</li>
  <li>Uses the <a href="../libnc/index.html">LibNC</a> library for simple tensor manipulation using the
  C language.</li>
  <li>Simple command line tools (<b>ts_test</b>, <b>ts_sd</b>, <b>ts_chat</b>, <b>ts_audiototext</b> are provided to test the various models).</li>
</ul>
</p>
The free version is released as binary code for non-commercial use
only. Please contact <a href="../index.html">
<script>
function f(s)
{
    var a, i;
    a = "";
    for(i = 0; i < s.length; i++)
        a += String.fromCharCode(s.charCodeAt(i) - 1);
    return a;
}
document.write(f("gbcsjdf!bu!cfmmbse!epu!psh"));
</script></a> for commercial use.

<h2>Download</h2>
               
<ul>
  <li>Linux version <a href="ts_server_free-2025-03-09.tar.gz">ts_server_free-2025-03-09.tar.gz</a> (<a href="Changelog">Changelog</a>).</li>
  <li>Windows version <a href="ts_server_free-2024-09-30-win64.zip">ts_server_free-2024-09-30-win64.zip</a> (<a href="Changelog">Changelog</a>).</li>
</ul>

<h2><a href="ts_server.html">Documentation</a></h2>

<h2>Benchmarks</h2>

<h4>Text generation</h4>
<p>
  100 tokens are generated with a batch size of 1 and 50 input tokens:
</p>
    <table class="sortable">
      <thead>
        <tr>
          <th aria-sort="ascending" style="width:15em;">Model<sup>(3)</sup>
          <th class="num" style="width:8em;">Epyc 7313 (6)<br>(tokens/s)
          <th class="num" style="width:8em;">RTX A6000<br>(tokens/s)
          <th class="num" style="width:8em;">RTX 4090<br>(tokens/s)
        </tr>
      </thead>
      <tbody>
        <tr><td>gptj_6B_q4<td class="num">21.5<td class="num">132<td class="num">164</tr>
        <tr><td>flan_t5_xxl_q4<td class="num">25<td class="num">130<td class="num">158</tr>
        <tr><td>llama2_7B_q4<td class="num">23<td class="num">115<td class="num">144</tr>
        <tr><td>llama2_13B_q4<td class="num">12.0<td class="num">69.3<td class="num">88</tr>
        <tr><td>gptneox_20B_q4<td class="num">8.1<td class="num">45.5<td class="num">59</tr>
        <tr><td>llama2_70B_q4<td class="num">2.5<td class="num">15.2<td class="num">-</tr>
      </tbody>
    </table>

<p>
  8 simultaneous requests generating 100 tokens with 50 input tokens
  (equivalent to a batch size of 8):
</p>
    <table class="sortable">
      <thead>
        <tr>
          <th aria-sort="ascending" style="width:15em;">Model<sup>(3)</sup>
          <th class="num" style="width:8em;">RTX A6000<br>(tokens/s)
        </tr>
      </thead>
      <tbody>
        <tr><td>llama2_7B_q4<td class="num">783</tr>
        <tr><td>llama2_13B_q4<td class="num">492</tr>
        <tr><td>llama2_70B_q4<td class="num">118</tr>
      </tbody>
    </table>
    
<h4>Text to image</h4>
<p>
  A single 512x512 image is generated using 50 time steps.
</p>

    <table class="sortable">
      <thead>
        <tr>
          <th aria-sort="ascending" style="width:15em;">Model<sup>(3)</sup>
          <th class="num" style="width:7em;">RTX A6000<br>(seconds)
          <th class="num" style="width:7em;">RTX 4090<br>(seconds)
        </tr>
      </thead>
      <tbody>
        <tr><td>stable diffusion 1.4<td class="num">1.82<td class="num">1.21</tr>
        <tr><td>stable diffusion 2.1<td class="num">1.67<td class="num">1.19</tr>
      </tbody>
    </table>
    
<h2><a name="models">Available Models</a></h2>

We provide here model files that can be used with the TextSynth
Server. Each model was evaluated with
the <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>
with the TextSynth server on a single RTX A6000 GPU.
<p>Language Models:

<table class="sortable" id="models">
  <thead>
    <tr>
      <th style="width:16em;"><button>Model<sup>(1)(2)(3)</sup><span aria-hidden="true"></span><br></button>
      <th class="num" style="width:7em;"><button>Size<sup>(4)</sup><span aria-hidden="true"></span><br>(GB)</button>
      <th class="num" style="width:8em;"><button>lambada<sup>(5)</sup><span aria-hidden="true"></span><br>(ppl)</button>
      <th class="num" style="width:8em;"><button>lambada<span aria-hidden="true"></span><br>(acc)</button>
      <th class="num" style="width:8em;"><button>hellaswag<span aria-hidden="true"></span><br>(acc_norm)</button>
      <th class="num" style="width:8.5em;"><button>winogrande<span aria-hidden="true"></span><br>(acc)</button>
      <th class="num" style="width:8em;"><button>piqa<span aria-hidden="true"></span><br>(acc)</button>
      <th class="num" style="width:8em;"><button>coqa<span aria-hidden="true"></span><br>(f1)</button>
      <th class="num" style="width:8em;" aria-sort="descending"><button>average<span aria-hidden="true"></span><br></button>
    </tr>
  </thead>
  <tbody>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/bloom_560M.bin">bloom_560M</a>
<td class="num">    1.1<td class="num">     29.176<td class="num">      36.8%<td class="num">      35.8%<td class="num">      51.4%<td class="num">      63.7%<td class="num">      36.0%<td class="num">      44.7%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/codegen_6B_mono_q4.bin">codegen_6B_mono_q4</a>
<td class="num">    4.4<td class="num">     69.409<td class="num">      28.0%<td class="num">      35.7%<td class="num">      51.1%<td class="num">      60.2%<td class="num">      38.0%<td class="num">      42.6%</tr>
<tr><td>codegen_6B_mono_q8
<td class="num">    7.7<td class="num">     67.262<td class="num">      28.1%<td class="num">      35.8%<td class="num">      50.8%<td class="num">      60.1%<td class="num">      39.1%<td class="num">      42.8%</tr>
<td>fairseq_gpt_13B       <td class="num">   26.2<td class="num">      3.567<td class="num">      71.9%<td class="num">      72.7%<td class="num">      67.5%<td class="num">      77.6%<td class="num">      70.1%<td class="num">      71.9%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/fairseq_gpt_13B_q4.bin">fairseq_gpt_13B_q4</a>
<td class="num">    7.9<td class="num">      3.646<td class="num">      71.2%<td class="num">      72.5%<td class="num">      67.6%<td class="num">      77.4%<td class="num">      70.6%<td class="num">      71.9%</tr>
<tr><td>fairseq_gpt_13B_q8
<td class="num">   14.2<td class="num">      3.565<td class="num">      71.8%<td class="num">      72.7%<td class="num">      67.2%<td class="num">      77.7%<td class="num">      70.0%<td class="num">      71.9%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/flan_t5_base.bin">flan_t5_base</a>
<td class="num">    0.5<td class="num">     12.891<td class="num">      54.2%<td class="num">      36.5%<td class="num">      54.7%<td class="num">      65.8%<td class="num">      62.1%<td class="num">      54.7%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/flan_t5_base_q8.bin">flan_t5_base_q8</a>
<td class="num">    0.3<td class="num">     13.098<td class="num">      54.2%<td class="num">      36.4%<td class="num">      54.2%<td class="num">      65.7%<td class="num">      61.8%<td class="num">      54.5%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/flan_t5_small.bin">flan_t5_small</a>
<td class="num">    0.2<td class="num">     23.343<td class="num">      46.7%<td class="num">      29.2%<td class="num">      50.0%<td class="num">      62.4%<td class="num">      47.9%<td class="num">      47.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/flan_t5_small_q8.bin">flan_t5_small_q8</a>
<td class="num">    0.1<td class="num">     23.449<td class="num">      46.7%<td class="num">      29.2%<td class="num">      49.7%<td class="num">      62.4%<td class="num">      48.2%<td class="num">      47.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/flan_t5_xxl_q4.bin">flan_t5_xxl_q4</a>
<td class="num">    6.5<td class="num">      3.010<td class="num">      77.7%<td class="num">      71.5%<td class="num">      73.4%<td class="num">      77.6%<td class="num">      71.8%<td class="num">      74.4%</tr>
<tr><td>flan_t5_xxl_q8
<td class="num">   12.0<td class="num">      3.049<td class="num">      77.8%<td class="num">      72.1%<td class="num">      75.1%<td class="num">      77.8%<td class="num">      73.1%<td class="num">      75.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/flan_ul2_20B_q4.bin">flan_ul2_20B_q4</a>
<td class="num">   11.3<td class="num">          -<td class="num">      74.1%<td class="num">      24.3%<td class="num">      51.1%<td class="num">      49.9%<td class="num">      78.8%<td class="num">      55.6%</tr>
<tr><td>flan_ul2_20B_q8
<td class="num">   20.9<td class="num">          -<td class="num">      74.4%<td class="num">      24.4%<td class="num">      52.0%<td class="num">      50.6%<td class="num">      77.3%<td class="num">      55.7%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gpt2_117M.bin">gpt2_117M</a>
<td class="num">    0.3<td class="num">     40.110<td class="num">      32.9%<td class="num">      31.1%<td class="num">      52.1%<td class="num">      62.9%<td class="num">      27.3%<td class="num">      41.3%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gpt2_345M.bin">gpt2_345M</a>
<td class="num">    0.7<td class="num">     18.272<td class="num">      43.5%<td class="num">      39.4%<td class="num">      53.3%<td class="num">      67.7%<td class="num">      43.1%<td class="num">      49.4%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gpt2_345M_q8.bin">gpt2_345M_q8</a>
<td class="num">    0.5<td class="num">     18.452<td class="num">      43.1%<td class="num">      39.4%<td class="num">      53.1%<td class="num">      67.5%<td class="num">      41.9%<td class="num">      49.0%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gpt2_774M.bin">gpt2_774M</a>
<td class="num">    1.6<td class="num">     12.966<td class="num">      47.8%<td class="num">      45.4%<td class="num">      55.6%<td class="num">      70.4%<td class="num">      48.5%<td class="num">      53.5%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gpt2_774M_q8.bin">gpt2_774M_q8</a>
<td class="num">    1.0<td class="num">     12.928<td class="num">      47.9%<td class="num">      45.4%<td class="num">      55.3%<td class="num">      70.3%<td class="num">      48.2%<td class="num">      53.4%</tr>
<tr><td>gpt2_1558M            <td class="num">    3.1<td class="num">     10.637<td class="num">      51.3%<td class="num">      50.8%<td class="num">      58.4%<td class="num">      70.8%<td class="num">      53.2%<td class="num">      56.9%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gpt2_1558M_q8.bin">gpt2_1558M_q8</a>
<td class="num">    1.8<td class="num">     10.655<td class="num">      51.2%<td class="num">      50.8%<td class="num">      58.6%<td class="num">      70.8%<td class="num">      53.2%<td class="num">      56.9%</tr>

<td>gptj_6B               <td class="num">   12.1<td class="num">      4.124<td class="num">      69.0%<td class="num">      66.2%<td class="num">      64.8%<td class="num">      75.5%<td class="num">      66.9%<td class="num">      68.5%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gptj_6B_q4.bin">gptj_6B_q4</a>
<td class="num">    3.8<td class="num">      4.153<td class="num">      68.9%<td class="num">      65.7%<td class="num">      63.9%<td class="num">      74.4%<td class="num">      67.0%<td class="num">      68.0%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gptj_6B_q8.bin">gptj_6B_q8</a>
<td class="num">    6.6<td class="num">      4.122<td class="num">      69.1%<td class="num">      66.2%<td class="num">      64.4%<td class="num">      75.4%<td class="num">      66.4%<td class="num">      68.3%</tr>
<td>gptneox_20B           <td class="num">   41.1<td class="num">      3.657<td class="num">      72.6%<td class="num">      71.4%<td class="num">      65.5%<td class="num">      77.5%<td class="num">      73.3%<td class="num">      72.0%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gptneox_20B_q4.bin">gptneox_20B_q4</a>
<td class="num">   12.2<td class="num">      3.711<td class="num">      72.0%<td class="num">      69.3%<td class="num">      64.8%<td class="num">      76.7%<td class="num">      70.8%<td class="num">      70.7%</tr>
<tr><td>gptneox_20B_q8
<td class="num">   22.1<td class="num">      3.659<td class="num">      72.6%<td class="num">      71.3%<td class="num">      65.8%<td class="num">      77.3%<td class="num">      72.9%<td class="num">      72.0%</tr>

<tr><td>llama_7B              <td class="num">   13.5<td class="num">      3.463<td class="num">      73.6%<td class="num">      76.2%<td class="num">      70.4%<td class="num">      78.1%<td class="num">      75.4%<td class="num">      74.7%</tr>
<tr><td>llama_7B_q4           <td class="num">    4.0<td class="num">      3.549<td class="num">      73.2%<td class="num">      75.5%<td class="num">      70.4%<td class="num">      78.0%<td class="num">      74.7%<td class="num">      74.4%</tr>
<tr><td>llama_7B_q8           <td class="num">    7.3<td class="num">      3.453<td class="num">      73.7%<td class="num">      76.1%<td class="num">      70.2%<td class="num">      78.0%<td class="num">      75.5%<td class="num">      74.7%</tr>
<tr><td>llama_13B_q4          <td class="num">    7.6<td class="num">      3.130<td class="num">      77.1%<td class="num">      78.6%<td class="num">      72.2%<td class="num">      78.3%<td class="num">      77.8%<td class="num">      76.8%</tr>
<tr><td>llama_13B_q8          <td class="num">   14.0<td class="num">      3.178<td class="num">      76.5%<td class="num">      79.1%<td class="num">      73.2%<td class="num">      79.1%<td class="num">      77.1%<td class="num">      77.0%</tr>
<tr><td>llama_30B_q4          <td class="num">   18.7<td class="num">      2.877<td class="num">      77.5%<td class="num">      82.4%<td class="num">      75.7%<td class="num">      80.2%<td class="num">      80.2%<td class="num">      79.2%</tr>
<tr><td>llama_30B_q8          <td class="num">   34.8<td class="num">      2.853<td class="num">      77.7%<td class="num">      82.7%<td class="num">      76.3%<td class="num">      80.3%<td class="num">      80.4%<td class="num">      79.5%</tr>
<tr><td>llama_65B_q4          <td class="num">   37.2<td class="num">      2.760<td class="num">      78.5%<td class="num">      83.9%<td class="num">      76.6%<td class="num">      81.4%<td class="num">      83.2%<td class="num">      80.7%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/opt_125M.bin">opt_125M</a>
<td class="num">    0.3<td class="num">     26.028<td class="num">      37.9%<td class="num">      31.3%<td class="num">      50.2%<td class="num">      63.2%<td class="num">      23.4%<td class="num">      41.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/opt_30B_q4.bin">opt_30B_q4</a>
<td class="num">   17.8<td class="num">      3.656<td class="num">      71.5%<td class="num">      72.1%<td class="num">      68.0%<td class="num">      77.4%<td class="num">      69.9%<td class="num">      71.8%</tr>
<tr><td>opt_30B_q8
<td class="num">   32.6<td class="num">      3.628<td class="num">      71.6%<td class="num">      72.3%<td class="num">      68.2%<td class="num">      77.7%<td class="num">      71.4%<td class="num">      72.3%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/opt_66B_q4.bin">opt_66B_q4</a>
<td class="num">   38.2<td class="num">      3.308<td class="num">      73.4%<td class="num">      74.4%<td class="num">      68.4%<td class="num">      78.5%<td class="num">      75.0%<td class="num">      73.9%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_70M.bin">pythia_deduped_70M</a>
<td class="num">    0.1<td class="num">     96.126<td class="num">      25.6%<td class="num">      28.3%<td class="num">      54.4%<td class="num">      60.4%<td class="num">      13.1%<td class="num">      36.3%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_160M.bin">pythia_deduped_160M</a>
<td class="num">    0.3<td class="num">     26.380<td class="num">      36.9%<td class="num">      32.3%<td class="num">      51.4%<td class="num">      63.8%<td class="num">      23.2%<td class="num">      41.5%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_410M.bin">pythia_deduped_410M</a>
<td class="num">    0.8<td class="num">     10.827<td class="num">      51.7%<td class="num">      40.8%<td class="num">      54.0%<td class="num">      67.2%<td class="num">      43.0%<td class="num">      51.4%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_410M_q8.bin">pythia_deduped_410M_q8</a>
<td class="num">    0.5<td class="num">     10.729<td class="num">      51.8%<td class="num">      40.7%<td class="num">      53.8%<td class="num">      67.1%<td class="num">      42.7%<td class="num">      51.2%</tr>
<tr><td>pythia_deduped_1B     <td class="num">    2.0<td class="num">      7.273<td class="num">      58.5%<td class="num">      49.0%<td class="num">      54.5%<td class="num">      71.0%<td class="num">      49.9%<td class="num">      56.6%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_1B_q8.bin">pythia_deduped_1B_q8</a>
<td class="num">    1.2<td class="num">      7.286<td class="num">      58.4%<td class="num">      49.0%<td class="num">      54.9%<td class="num">      70.9%<td class="num">      49.0%<td class="num">      56.5%</tr>
<tr><td>pythia_deduped_1.4B   <td class="num">    2.8<td class="num">      6.546<td class="num">      63.1%<td class="num">      52.2%<td class="num">      57.1%<td class="num">      72.7%<td class="num">      52.6%<td class="num">      59.5%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_1.4B_q8.bin">pythia_deduped_1.4B_q8</a>
<td class="num">    1.6<td class="num">      6.577<td class="num">      63.3%<td class="num">      52.1%<td class="num">      55.7%<td class="num">      73.1%<td class="num">      53.0%<td class="num">      59.4%</tr>
<tr><td>pythia_deduped_2.8B   <td class="num">    5.6<td class="num">      4.787<td class="num">      67.1%<td class="num">      61.6%<td class="num">      60.9%<td class="num">      74.4%<td class="num">      65.5%<td class="num">      65.9%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_2.8B_q8.bin">pythia_deduped_2.8B_q8</a>
<td class="num">    3.1<td class="num">      4.778<td class="num">      66.9%<td class="num">      61.5%<td class="num">      61.2%<td class="num">      74.5%<td class="num">      65.6%<td class="num">      66.0%</tr>
<td>pythia_deduped_6.9B   <td class="num">   13.7<td class="num">      4.195<td class="num">      69.1%<td class="num">      65.7%<td class="num">      63.9%<td class="num">      75.1%<td class="num">      66.1%<td class="num">      68.0%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_6.9B_q4.bin">pythia_deduped_6.9B_q4</a>
<td class="num">    4.3<td class="num">      4.344<td class="num">      68.3%<td class="num">      65.0%<td class="num">      62.5%<td class="num">      75.3%<td class="num">      66.3%<td class="num">      67.5%</tr>
<tr><td>pythia_deduped_6.9B_q8
<td class="num">    7.5<td class="num">      4.187<td class="num">      69.4%<td class="num">      65.7%<td class="num">      63.6%<td class="num">      75.5%<td class="num">      66.8%<td class="num">      68.2%</tr>
<tr><td>pythia_deduped_12B    <td class="num">   23.7<td class="num">      3.854<td class="num">      70.9%<td class="num">      69.2%<td class="num">      63.9%<td class="num">      76.3%<td class="num">      70.8%<td class="num">      70.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/pythia_deduped_12B_q4.bin">pythia_deduped_12B_q4</a>
<td class="num">    7.2<td class="num">      4.187<td class="num">      69.2%<td class="num">      68.5%<td class="num">      63.1%<td class="num">      76.4%<td class="num">      69.6%<td class="num">      69.4%</tr>
<tr><td>pythia_deduped_12B_q8
<td class="num">   12.8<td class="num">      3.857<td class="num">      70.9%<td class="num">      69.2%<td class="num">      64.2%<td class="num">      76.1%<td class="num">      70.9%<td class="num">      70.3%</tr>

<tr><td>rwkv_14B              <td class="num">   28.3<td class="num">      3.819<td class="num">      71.6%<td class="num">      70.2%<td class="num">      63.1%<td class="num">      77.5%<td class="num">      47.2%<td class="num">      65.9%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/rwkv_14B_q4.bin">rwkv_14B_q4</a>
<td class="num">    8.5<td class="num">      4.076<td class="num">      68.3%<td class="num">      69.8%<td class="num">      63.1%<td class="num">      77.1%<td class="num">      45.0%<td class="num">      64.7%</tr>
<tr><td>rwkv_14B_q8
<td class="num">   15.3<td class="num">      3.806<td class="num">      71.9%<td class="num">      70.2%<td class="num">      63.0%<td class="num">      77.5%<td class="num">      47.1%<td class="num">      65.9%</tr>
<tr><td>rwkv_7B               <td class="num">   16<td class="num">      4.396<td class="num">      67.5%<td class="num">      65.6%<td class="num">      61.9%<td class="num">      75.6%<td class="num">      39.7%<td class="num">      62.1%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/rwkv_7B_q4.bin">rwkv_7B_q4</a>
<td class="num">    4.6<td class="num">      4.939<td class="num">      64.7%<td class="num">      64.8%<td class="num">      61.2%<td class="num">      75.4%<td class="num">      38.4%<td class="num">      60.9%</tr>
<tr><td>rwkv_7B_q8
<td class="num">    8.0<td class="num">      4.395<td class="num">      67.5%<td class="num">      65.6%<td class="num">      61.6%<td class="num">      75.9%<td class="num">      40.2%<td class="num">      62.2%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/RedPajama-INCITE-7B_q4.bin">RedPajama-INCITE-7B_q4</a>
<td class="num">    4.3<td class="num">      4.006<td class="num">      71.0%<td class="num">      69.7%<td class="num">      64.6%<td class="num">      76.3%<td class="num">      71.7%<td class="num">      70.7%</tr>
<tr><td>RedPajama-INCITE-7B_q8
<td class="num">    7.5<td class="num">      3.910<td class="num">      71.4%<td class="num">      70.4%<td class="num">      64.3%<td class="num">      77.0%<td class="num">      71.9%<td class="num">      71.0%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/falcon_40B_q4.bin">falcon_40B_q4</a>
<td class="num">   24.6<td class="num">      2.844<td class="num">      77.6%<td class="num">      82.5%<td class="num">      76.2%<td class="num">      82.2%<td class="num">      78.8%<td class="num">      79.5%</tr>
<tr><td>falcon_40B_q8
<td class="num">   45.0<td class="num">      2.799<td class="num">      77.9%<td class="num">      82.7%<td class="num">      76.7%<td class="num">      82.2%<td class="num">      80.4%<td class="num">      80.0%</tr>
<tr><td>falcon_7B
<td class="num">   14.4<td class="num">      3.359<td class="num">      75.0%<td class="num">      76.2%<td class="num">      67.3%<td class="num">      79.4%<td class="num">      72.1%<td class="num">      74.0%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/falcon_7B_q4.bin">falcon_7B_q4</a>
<td class="num">    4.6<td class="num">      3.444<td class="num">      73.9%<td class="num">      75.8%<td class="num">      67.5%<td class="num">      79.7%<td class="num">      71.6%<td class="num">      73.7%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/falcon_7B_q8.bin">falcon_7B_q8</a>
<td class="num">    7.9<td class="num">      3.368<td class="num">      75.0%<td class="num">      76.2%<td class="num">      66.9%<td class="num">      79.5%<td class="num">      71.9%<td class="num">      73.9%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/mpt_30B_q4.bin">mpt_30B_q4</a>            <td class="num">   17.8<td class="num">      3.219<td class="num">      78.9%<td class="num">      79.4%<td class="num">      70.1%<td class="num">      79.8%<td class="num">      79.8%<td class="num">      77.6%</tr>
<tr><td>mpt_30B_q8            <td class="num">   32.6<td class="num">      3.062<td class="num">      80.7%<td class="num">      79.8%<td class="num">      70.7%<td class="num">      80.0%<td class="num">      79.9%<td class="num">      78.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/mpt_7B_q4.bin">mpt_7B_q4</a>             <td class="num">    4.3<td class="num">      3.949<td class="num">      73.1%<td class="num">      75.7%<td class="num">      67.4%<td class="num">      79.0%<td class="num">      75.9%<td class="num">      74.2%</tr>
<tr><td>mpt_7B_q8             <td class="num">    7.5<td class="num">      3.850<td class="num">      73.2%<td class="num">      76.2%<td class="num">      68.5%<td class="num">      79.1%<td class="num">      76.4%<td class="num">      74.7%</tr>

<tr><td>llama2_7B             <td class="num">   13.5<td class="num">      3.428<td class="num">      74.5%<td class="num">      76.2%<td class="num">      69.7%<td class="num">      78.4%<td class="num">      77.2%<td class="num">      75.2%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama2_7B_q4.bin">llama2_7B_q4</a>          <td class="num">  4.0<td class="num">      3.487<td class="num">      73.5%<td class="num">      75.5%<td class="num">      69.9%<td class="num">      77.6%<td class="num">      77.8%<td class="num">      74.9%</tr>
<tr><td>llama2_13B            <td class="num">   26.0<td class="num">      3.051<td class="num">      77.2%<td class="num">      79.6%<td class="num">      72.1%<td class="num">      78.9%<td class="num">      79.3%<td class="num">      77.4%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama2_13B_q4.bin">llama2_13B_q4</a>         <td class="num">  7.6<td class="num">      3.109<td class="num">      77.0%<td class="num">      79.0%<td class="num">      72.6%<td class="num">      79.5%<td class="num">      78.9%<td class="num">      77.4%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama2_70B_q4.bin">llama2_70B_q4</a>         <td class="num">   39.3<td class="num">      2.646<td class="num">      80.6%<td class="num">      84.0%<td class="num">      78.7%<td class="num">      82.0%<td class="num">      83.4%<td class="num">      81.7%</tr>

<tr><td>llama2_7B_q3          <td class="num">  3.2<td class="num">      3.566<td class="num">      72.7%<td class="num">      74.1%<td class="num">      68.0%<td class="num">      77.6%<td class="num">      77.5%<td class="num">      74.0%</tr>
<tr><td>llama2_13B_q3         <td class="num">  6.1<td class="num">      3.148<td class="num">      76.5%<td class="num">      77.9%<td class="num">      71.4%<td class="num">      78.4%<td class="num">      77.8%<td class="num">      76.4%</tr>
<tr><td>llama2_70B_q3         <td class="num"> 30.8<td class="num">      2.638<td class="num">      79.9%<td class="num">      82.9%<td class="num">      77.7%<td class="num">      81.7%<td class="num">      82.6%<td class="num">      80.9%</tr>

<tr><td>mistral_7B
<td class="num"> 14.5<td class="num">      3.178<td class="num">      76.2%<td class="num">      81.0%<td class="num">      74.2%<td class="num">      80.4%<td class="num">      80.9%<td class="num">      78.5%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/mistral_7B_q4.bin">mistral_7B_q4</a>
<td class="num">  4.3<td class="num">      3.412<td class="num">      74.9%<td class="num">      80.1%<td class="num">      73.9%<td class="num">      80.7%<td class="num">      80.3%<td class="num">      78.0%</tr>
<tr><td>mistral_7B_q8
<td class="num">  7.8<td class="num">      3.174<td class="num">      76.0%<td class="num">      81.0%<td class="num">      73.6%<td class="num">      80.4%<td class="num">      80.7%<td class="num">      78.3%</tr>

<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/mixtral_47B_q3.bin">mixtral_47B_q3</a>
<td class="num"> 19.3<td class="num">      2.851<td class="num">      76.8%<td class="num">      82.2%<td class="num">      75.6%<td class="num">      81.3%<td class="num">      79.8%<td class="num">      79.1%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/mixtral_47B_q4.bin">mixtral_47B_q4</a>
<td class="num"> 26.5<td class="num">      2.811<td class="num">      78.6%<td class="num">      83.3%<td class="num">      76.0%<td class="num">      82.6%<td class="num">      80.4%<td class="num">      80.2%</tr>
<tr><td>mixtral_47B_q8
<td class="num"> 49.7<td class="num">      2.790<td class="num">      79.3%<td class="num">      83.9%<td class="num">      78.1%<td class="num">      82.0%<td class="num">      80.7%<td class="num">      80.8%</tr>

<tr><td>llama3_8B             <td class="num"> 16.1<td class="num">      3.107<td class="num">      76.8%<td class="num">      79.1%<td class="num">      73.1%<td class="num">      79.7%<td class="num">      80.7%<td class="num">      77.9%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama3_8B_q4.bin">llama3_8B_q4</a>          <td class="num">  5.5<td class="num">      3.291<td class="num">      75.2%<td class="num">      78.2%<td class="num">      73.5%<td class="num">      78.8%<td class="num">      80.4%<td class="num">      77.2%</tr>
<tr><td>llama3_70B            <td class="num">141.1<td class="num">      2.597<td class="num">      80.6%<td class="num">      84.9%<td class="num">      80.1%<td class="num">      82.3%<td class="num">      84.0%<td class="num">      82.4%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama3_70B_q4.bin">llama3_70B_q4</a>         <td class="num"> 41.7<td class="num">      2.619<td class="num">      80.4%<td class="num">      84.4%<td class="num">      80.3%<td class="num">      82.1%<td class="num">      83.1%<td class="num">      82.1%</tr>

<tr><td>llama3.1_8B           <td class="num"> 16.1<td class="num">      3.150<td class="num">      76.6%<td class="num">      78.8%<td class="num">      73.9%<td class="num">      79.9%<td class="num">      80.8%<td class="num">      78.0%</tr>
<tr><td>llama3.1_70B          <td class="num">141.1<td class="num">      2.670<td class="num">      80.1%<td class="num">      84.9%<td class="num">      79.4%<td class="num">      83.0%<td class="num">      83.7%<td class="num">      82.2%</tr>

<tr><td>llama3.1_70B_q4       <td class="num"> 41.8<td class="num">      2.713<td class="num">      79.9%<td class="num">      84.4%<td class="num">      79.4%<td class="num">      82.6%<td class="num">      83.4%<td class="num">      81.9%</tr>

<tr><td>llama3.1_70B_q3       <td class="num"> 31.1<td class="num">      2.865<td class="num">      78.0%<td class="num">      83.0%<td class="num">      78.4%<td class="num">      82.0%<td class="num">      83.6%<td class="num">      81.0%</tr>

<tr><td>llama3.1_405B_q4      <td class="num">232.4<td class="num">      2.454<td class="num">      81.6%<td class="num">      87.0%<td class="num">      82.4%<td class="num">      83.8%<td class="num">      83.8%<td class="num">      83.7%</tr>

<tr><td>qwen2_7B              <td class="num"> 15.2<td class="num">      3.647<td class="num">      72.3%<td class="num">      78.3%<td class="num">      72.3%<td class="num">      79.9%<td class="num">      80.9%<td class="num">      76.8%</tr>
<tr><td>qwen2_7B_q4           <td class="num">  5.3<td class="num">      3.712<td class="num">      72.0%<td class="num">      77.8%<td class="num">      71.3%<td class="num">      79.7%<td class="num">      81.7%<td class="num">      76.5%</tr>

</tbody>
</table>
</p>

<p>Chat Models:

<table class="sortable" id="chat_models">
  <thead>
    <tr>
      <th style="width:20em;"><button>Model<sup>(1)(2)(3)</sup><span aria-hidden="true"></span><br></button>
      <th class="num" style="width:7em;"><button>Size<sup>(4)</sup><span aria-hidden="true"></span><br>(GB)</button>
      <th aria-sort="descending" class="num" style="width:12em;"><button>MMLU<sup>(7)</sup><span aria-hidden="true"></span><br>(exact_match)</button>
    </tr>
  </thead>
  <tbody>
    <tr><td>llama3_8B_instruct    <td class="num"> 16.1<td class="num">      67.3%</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama3_8B_instruct_q4.bin">llama3_8B_instruct_q4</a><td class="num">5.5<td class="num">65.7%</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama2_7B_chat_q4.bin">llama2_7B_chat_q4</a><td class="num">3.9<td class="num">45.3%</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama2_13B_chat_q4.bin">llama2_13B_chat_q4</a><td class="num">7.6<td class="num">51.2%</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama2_70B_chat_q4.bin">llama2_70B_chat_q4</a><td class="num">39.3<td class="num">61.1%</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/mistral_7B_instruct_q4.bin">mistral_7B_instruct_q4</a><td class="num">3.9<td class="num">53.0%</tr>
    <tr><td>mixtral_47B_instruct_q4<td class="num"> 26.5<td class="num">      67.6%</tr>
    <tr><td>llama3.1_8B_instruct  <td class="num"> 16.1<td class="num">      68.6%</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/llama3.1_8B_instruct_q4.bin">llama3.1_8B_instruct_q4</a><td class="num">  5.6<td class="num">      67.1%</tr>
    <tr><td>llama3.1_70B_instruct_q4<td class="num"> 41.8<td class="num">      82.4%</tr>
<tr><td>phi3_mini_4k_instruct <td class="num">  7.6<td class="num">      70.1%</tr>
<tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/phi3_mini_4k_instruct_q4.bin">phi3_mini_4k_instruct_q4</a><td class="num">  2.3<td class="num">      67.8%</tr>
<tr><td>phi3.5_mini_instruct  <td class="num">  7.7<td class="num">      67.7%</tr>
<tr><td>phi3.5_mini_instruct_q4<td class="num">  2.4<td class="num">      65.9%</tr>
<tr><td>qwen2_7B_instruct     <td class="num"> 15.2<td class="num">      70.3%</tr>
<tr><td>qwen2_7B_instruct_q4  <td class="num">  5.3<td class="num">      68.7%</tr>
<tr><td>llama3.3_70B_instruct_q4<td class="num"> 41.8<td class="num">      81.9%</tr>
  </tbody>
</table>
  
<p>
Translation Models:

<table class="sortable">
  <thead>
    <tr>
      <th aria-sort="ascending" style="width:15em;"><button>Model<sup>(1)(2)(3)</sup><span aria-hidden="true"></span><br></button>
      <th class="num" style="width:7em;"><button>Size<sup>(4)</sup><span aria-hidden="true"></span><br>(GB)</button>
      <th style="width:30em;">Description
    </tr>
  </thead>
  <tbody>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/m2m100_1_2B_q8.bin">m2m100_1_2B_q8</a><td class="num">1.6<td>Translation between 100 languages</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/nllb200_1.3B_q8.bin">nllb200_1.3B_q8</a><td class="num">2.0<td>Translation between 200 languages</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/nllb200_3.3B_q8.bin">nllb200_3.3B_q8</a><td class="num">4.6<td>Translation between 200 languages</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/madlad400_7B_q4.bin">madlad400_7B_q4</a><td class="num">5.7<td>Translation between 400 languages</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/madlad400_3B_q4.bin">madlad400_3B_q4</a><td class="num">2.2<td>Translation between 400 languages</tr>
  </tbody>
</table>
</p>
<p>
Embeddings Models:

<table class="sortable">
  <thead>
    <tr>
      <th aria-sort="ascending" style="width:15em;"><button>Model<sup>(1)(2)(3)</sup><span aria-hidden="true"></span><br></button>
      <th class="num" style="width:7em;"><button>Size<sup>(4)</sup><span aria-hidden="true"></span><br>(GB)</button>
      <th style="width:30em;">Description
    </tr>
  </thead>
  <tbody>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/gte_qwen2_1.5B_instruct_q8.bin">gte_qwen2_1.5B_instruct_q8</a><td class="num">1.9<td>Qwen2 GTE embeddings</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/bge_large_en_v1.5_q8.bin">bge_large_en_v1.5_q8</a><td class="num">0.4<td>BGE-Large EN v1.5 embeddings</tr>
  </tbody>
</table>
</p>
<p>
Text-to-Image Models:

<table class="sortable">
  <thead>
    <tr>
      <th aria-sort="ascending" style="width:15em;"><button>Model<sup>(1)(2)(3)</sup><span aria-hidden="true"></span><br></button>
      <th class="num" style="width:7em;"><button>Size<sup>(4)</sup><span aria-hidden="true"></span><br>(GB)</button>
      <th style="width:30em;">Description
    </tr>
  </thead>
  <tbody>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/sd_v1.4.bin">sd_v1.4</a><td class="num">2.1<td>Stable Diffusion text-to-image version 1.4</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/sd_v2.1.bin">sd_v2.1</a><td class="num">2.6<td>Stable Diffusion text-to-image version 2.1</tr>
  </tbody>
</table>
</p>
<p>
Audio Models:

<table class="sortable">
  <thead>
    <tr>
      <th aria-sort="ascending" style="width:15em;"><button>Model<sup>(1)(2)(3)</sup><span aria-hidden="true"></span><br></button>
      <th class="num" style="width:7em;"><button>Size<sup>(4)</sup><span aria-hidden="true"></span><br>(GB)</button>
      <th style="width:30em;">Description
    </tr>
  </thead>
  <tbody>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/whisper_large_v3_q8.bin">whisper_large_v3_q8</a><td class="num">1.8<td>Whisper large v3 speech-to-text transcription</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/parler_tts_large_v1_q8.bin">parler_tts_large_v1_q8</a><td class="num">1.1<td>Parler-TTS text-to-speech model</tr>
    <tr><td><a href="https://huggingface.co/fbellard/ts_server/resolve/main/dac_mono.bin">dac_mono</a><td class="num">0.3<td>Descript Audio Codec (used with Parler-TTS)</tr>
  </tbody>
</table>
</p>
<p>
  SHA256 of all the models: <a href="sha256.txt">sha256.txt</a>.
</p>
<p>
  Notes:
  <ol>
    <li>Some models have restrictive licenses. In particular, OPT,
    Vicuna and NLLB200 cannot be used commercially. BLOOM, Stable
    Diffusion, Llama 2, Llama 3, Llama 3.1 can be used commercially
    but have use limitations.</li>
    <li>For the larger models we don't provide the unquantized version when it is too large for consumer GPUs or when the quantized version gives the same performance as the unquantized version.</li>
    <li>The <b>q8</b> suffix indicates that the model was 8 bit
    quantized. The <b>q4</b> suffix indicates that the model was 4 bit
    quantized. The <b>q3</b> suffix indicates that the model was 3 bit
    quantized. Unquantized models use either float16 or bfloat16
    parameters.</li>
    <li>File size on disk (1 GB = 10<sup>9</sup> bytes). The amount of
      CPU or GPU RAM needed to run the model is close to this value.</li>
    <li>lambada perplexity (ppl) are comparable only for models
      using the same tokenizer. So the lambada accuracy (acc) should be used
      when comparing all models.</li>
    <li>The speed is measured on an AMD Epyc 7313 CPU using 16 threads (<tt>ts_test -T 16</tt>)</li>
    <li>MMLU was evaluated using 5 shots.</li>
  </ol>
</p>

<hr>
Fabrice Bellard - <a href="../index.html">https://bellard.org/</a>
</body>
</html>
