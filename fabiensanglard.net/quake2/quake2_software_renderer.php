<script type="text/javascript">
  var disqus_identifier = "quake2";
</script>
<!DOCTYPE html>
<html>
	<head>	
		<meta http-equiv="content-type" content="text/html; charset=utf-8">
		<meta name="Author" content="Fabien Sanglard">
		
		<meta name="Keywords" content="Quake 2 Source Code Review"/>
		<meta name="Description" content="Quake 2 Source Code Review"/>
		<meta name="viewport" content="width=device-width, initial-scale=1">
	

		
		

 
 	
		
		<title>Quake 2 Source Code Review</title>
		
	</head>
	<body>
		<div id="main">
           
					
			<link rel='stylesheet' href='../css/neo_style.css' type='text/css'  />



    <h1 id="site-name">
        <a  href="../index.html" >Fabien Sanglard's Website</a>
    </h1>

<script>
   function setEmailTitle()
	{
 		var folders = window.location.href.split("/"); 

		var currentFolder = folders[folders.length-2];

		var emailLink = document.getElementById("mail");

		emailLink.href = "mailto:fabiensanglard.net@gmail.com?subject="+currentFolder;

	}
	
	window.onload = setEmailTitle;
</script>
<style type='text/css'>
		/**
		 * Bulletproof syntax:
		 * http://www.fontspring.com/blog/further-hardening-of-the-bulletproof-syntax
		 * Font files generated by Font Squirrel:
		 * http://www.fontsquirrel.com
		 * License: Open Font License. See http://evenchick.com/wp-content/themes/blaskan/OFL.txt.
		 */
		@font-face {
			font-family: 'LeagueGothic';
			src: url('../font/league_gothic/league_gothic-webfont.eot'); /* IE9 Compat Modes */
			src: url('../font/league_gothic/league_gothic-webfont.eot?iefix') format('eot'), /* IE6-IE8 */
			     url('../font/league_gothic/league_gothic-webfont.woff') format('woff'), /* Modern Browsers */
			     url('../font/league_gothic/league_gothic-webfont.ttf')  format('truetype'), /* Safari, Android, iOS */
			     url('../font/league_gothic/league_gothic-webfont.svg') format('svg'); /* Legacy iOS */
		}
		
		@font-face {
			font-family: 'DejaVu Sans';
			src: url('../font/dejavu-sans/DejaVuSansMono.ttf'); /* IE9 Compat Modes */
			src: url('../font/dejavu-sans/DejaVuSansMono.ttf')  format('truetype') /* Safari, Android, iOS */
			     
		}

		
</style>

<header id="header" role="banner"><nav id="nav" role="navigation"><div class="menu">
	<ul id="menu-primary-navigation-1" >
         <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-91">
           <a href="../index.html" >Home</a>
         </li>
         <li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-95">
           <a href="../about/index.html">About</a>
         </li>
          <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-96">
           <a href="../faq/index.html">FAQ</a>
         </li>
         <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-92">
            <a id="mail" href="mailto:fabiensanglard.net@gmail.com?subject=Tunnel" title="Send me an email.">Email</a>
         </li>
         <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-93">
            <a href="../rss.xml" title="Suscribe to RSS Feed.">Rss</a>
         </li>
         <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-94">
            <a href="http://twitter.com/fabynou" title="Follow me on Twitter.">Twitter</a>
         </li>
     </ul></div></nav></header>
<!-- / #header -->
<section id="content" role="main">



		
<link rel="alternate" type="application/rss+xml" title="Fabien Sanglard &raquo; Feed" href="../rss.xml" />
<link rel="alternate" type="application/rss+xml" title="Fabien Sanglard &raquo; Comments Feed" href="../rss.xml" />
<div id="date">
       September 16th, 2011</div>
   <h1>Quake 2 Source Code Review 3/4</h1>
   <p id="paperbox">
	   	
          <a  href="index.php">
          <img src="quake2_icon.jpg" style="margin-left: 2ch;float:right; border:1px black solid;width:35%;">   
          </a> 
         
	<style> 

	table.credits { width:70%; a:link:color:rgb(0, 136, 204);}
	table.credits thead { background:transparent; }
	
	table.credits th { white-space:nowrap; }
	table.credits thead th { border-left:1px solid #ccc;  border-top:1px solid #ccc; padding:9px 9px 3px; color:#999; }
	table.credits tbody th,
	table.credits tbody td { border-top:1px solid #ccc; padding:6px 9px; }
	table.credits tbody th { padding:7px 0 7px 0; text-align:center; color:#999; }
	table.credits tbody th b { color:#333; font-weight:normal; }
	table.credits tbody td { border-left:1px solid #ccc; width:182px; }
	table.col3 tbody td { width:248px; }
	table.col2 tbody td { width:382px; }
	table.credits tbody .session { background:#d4e6fa url(session_bgblue.png) repeat-x 0 0; }
	table.credits tbody .session.alt { background:#e9ecf0 url(session_bggray.png) repeat-x 0 0; }
	table.credits tbody .session h3 { font-size:1em; margin-bottom:0; color:rgb(0, 136, 204); }
	table.credits tbody .session .hud-content { display:none; }
 
	
	blockquote.style1 
	{
  		
  		padding: 8px;
  		
  		width: 80%;
  		
  		background-color: #eeeeee;
  		border: 1px solid #dddddd;
  		
  		
  		margin: 5px;
  		background-image: url(images/openquote1.gif);
  		background-position: top left;
  		background-repeat: no-repeat;
  		text-indent: 23px;
  	
  	}
  	
  	blockquote.style1 span 
  	{
    	display: block;
    	font-style:italic;
    	background-image: url(images/closequote1.gif);
    	background-repeat: no-repeat;
    	background-position: bottom right;
    	text-align: justify;
  	}
	
	</style> 

Quake2 software render is the biggest, most complicated and hence most exciting module to explore.<br/>
<br/>
<a href="index.php">Quake 2 Source Code Review 1/4 (Intro) </a><br/>			
<a href="quake2Polymorphism.php">Quake 2 Source Code Review 2/4 (Polymorphism) </a><br/>
<a href="quake2_software_renderer.php">Quake 2 Source Code Review 3/4 (Software Renderer)</a><br/>
<a href="quake2_opengl_renderer.php">Quake 2 Source Code Review 4/4 (OpenGL Renderer)</a><br/>
<br/></p>
<div style="clear:both;"></div>

<h3>Software Renderer</h3>

<p id="paperbox">
<img src="../fd_proxy/quake2/softRendererStruct.png" style="float:left; margin: 0px 10px 10px 0px;"/>
 From disk to pixel there are no hidden mecanisms here:
 Everything has been carefully coded and optimized by hand. It is the last of its kind, marking the end of an era before the industry moved to hardware accelerated only.<br/>
 <br/>
The fundamental thing that differentiate the software and OpenGL renderer is the usage of a 256 colors palette system instead of the usual 24 bits RGB true color system found everywhere nowaday:<br/>
<br/>
<img width="732px" style="display: block; margin-left: auto; margin-right: auto;box-shadow: rgb(119, 119, 119) 3px 3px 3px;"  src="../fd_proxy/quake2/renderer.gif"/><br/>
<br/>
<p>
If we compare the hardware accelerated and the software renderer, the two most obvious differences are:<br/>
<ul>
	<li>Absence of colored lighting</li>
	<li>Absence of bilinear filtering</li>
</ul>
<p>
But except for those the engine managed to do amazing things via very clever arrangements of the palette that I detail later in this page:
<ul>
	<li>Color gradients (64 values) fast selection.</li>
	<li>Post-effect full screen color blending.</li>
	<li>Per-pixel translucency.</li>
</ul>
<br/>
<p>
The Quake2 palette is loaded first from the PAK archive file <code>pics/colormap.pcx</code>:<br/>
<br/>

<img style="display: block; margin-left: auto; margin-right: auto;box-shadow: rgb(119, 119, 119) 3px 3px 3px;" src="quake2_palette.jpg"/>
<br/>
<p>
<u><b>Note :</b></u> Black is at 0, White at 15, Green at 208 Red at the bottom left is 240 (transparent is 255).<br/>
<br/>
<p>
The first thing done is to use those 256 colors and re-arrange them as seen in <code>pics/colormap.pcx</code>:<br/>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto;box-shadow: rgb(119, 119, 119) 3px 3px 3px;"  src="../fd_proxy/quake2/COLORMAP.jpg"/>
<br/>
<p>
This 256x320 re-arrangement is used as lookup table and is incredibly clever, unlocking so many many sweat things:
<ul>
	<li>64 lines to fake color gradients: The first line is made of the 256 colors from the palette "unrolled". The 63 remaining entries in each column represent vertically
	    a gradient of the initial color, faked with the other 255 colors available. This enable a very simple and fast gradient selection mechanism: 
	    <ul>
	    	<li>First select a color from the palette within [0,255] (this comes from the color texture).</li>
		    <li>Then add x*256, where x is within [0,63] to get a darkening version of the color initially selected  (this comes from the lightmap texture).</li>
		</ul>
		You ended up having 64 shades of 256 colors. All this with only 256 colors from a static palette. Amazing.<br/>
		<br/>
	</li>
	<li>The rest of the image is made of 16x16 squares that enable palette based pixel blending. 
	You can clearly see all the squares made of a Source Color, a Destination Color and intermediate colors inbetween.
	That's how water translucency is done in the game. As and example the top left square blends between white and black.
	</li>
</ul>	
<p>
<br/>
<b><u>Trivia :</u></b> Quake2's software renderer was initially supposed to be RGB based instead of palette based thank to 
MMX technology as John Carmack announced it after the release of Quake1 in this video (at 10m17s mark):<br/>
<p><center><iframe width="425" height="349" src="https://www.youtube.com/embed/LXw6BkZ-gdY?t=10m17s" frameborder="0" allowfullscreen></iframe></center></p>
<br/>
<p>
MMX is a SIMD technology that would have allowed to work on the three channels of RGB at the cost of one 
channel hence enabling blending with an acceptable CPU consumption. My guess is that
 it was scrapped for two reasons:
 <ul>
 	<li>MMX enabled Pentium were rare in 1997, the software renderer was supposed to be the configuration for low budget machines.</li>
 	<li>Transfering an RGB (16 bits or 32 bits per pixel) instead of a palette based (8 bits per pixel) was consuming too much bandwidth and an acceptable framerate could be be achieved.</li>
 </ul>
</p>


</p>


</p>

<br/><br/>
<h3>Global architecture</h3>

<p id="paperbox">
Now that the big limitation is set (palette), we can go on the global architecture of the renderer. The philosophy is to rely heavily on what the 
Pentium is good at (floating point calculation) in order to minimize the impact of its weakness: Bus speed that affects writing pixels to memory. Most 
of the rendering path focuses on achieving ZERO overdraw.
In essence the software rendering path is identical to Quake 1's software renderer relying heavily 
on the BSP to traverse the map and the PVS (Potentially visible set) to build the set of polygons to render.
Three different things are rendered each frames:<br/>
<ul>
	<li>The map: Using a scan-line bsp based coherence algorithm (detailed later).</li>
	<li>Entities: Either as "sprite"(windows) or "alias"(players,enemies) using a plain scan-line algorithm.</li>
	<li>Particles.</li>
</ul>
<br/>
<p>
<u>Note :</u> If you are unfamiliar with those "old" algorithms, I highly recommend to read Chapter 3.6 and 15.6 from <a href="http://books.google.com/books?id=-4ngT05gmAQC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false">Computer graphics: principles and practice</a> By James D. Foley. You can also find a lot
of information in chapters 59 to 70 in  <a href="http://www.gamedev.net/page/resources/_/reference/programming/140/283/graphics-programming-black-book-r1698">Graphics programming black book</a> by Michael Abrash.<br/>
<br/>
Here it is a pseudo-highlevel-code:<br/>
</p>
<ol>
	<li>Render the map</li>
		<ul>
			<li>Walk the pre-processed BSP to determine in which cluster we are.</li>
			<li>Query the PVS database for this particular cluster: Retrieve and decompress the PVS.</li>
			<li>Using the PVS bit vector: Mark as visible every polygon belonging to a cluster designated as Potentially Visible.</li>
			<li>Walk  the BSP again, this time check if visible cluster are affected by any dynamic light. If so, mark them with the light's id number.</li>			
			<li>Now we have a list of what is potentially visible with lighting information:</li>
			<ul>
				<li>Use the BSP again, walk it near to far:</li>
					<ul>
						<li>Test against view frustrum and project all polygons on screen: Build a Global Edge Table.</li>
						<li>Render each face's surface (surface=texture+lightmap) to a cache in RAM.</li>
						<li>Insert surface pointer on the surface stack (later used for coherency).</li> 
					</ul>
				<li>Using the surface stack and the Global Edge Table, generate an Active Edge Table and  render the screen line by line from top to bottom. Use the surface cache in RAM as texture. Write spans to offscreen buffer and also populate a Z-Buffer.</li>
			</ul>
			
		</ul>
	<li>Render the entities, use the Z-Buffer to determine which part of the entity is visible.</li>
	<li>Render translucent textures, use a cool palette trick to calculate the translucent palette indices per pixel.</li>
	<li>Render particles.</li>
	<li>Perform post-effect (damage blend full screen with a red color).</li>	
</ol>

<br/>
<br/>
<u><b>Screen rendition :</b></u> Palette indices are written to an offscreen buffer. Depending if the engine runs fullscreen or windowed, DirectDraw or GDI are respectively used.
When the offscreen framebuffer has been completed, it is blitted to the videocard backbuffer (GDI=><code>rw_dib.c</code> DirectDraw=><code>rw_ddraw.c</code>) using either <code>WinGDI.h</code>'s <code>BitBlt</code> 
or <code>ddraw.h</code>'s <code>BltFast</code>.
</p>


<br/><br/>
<h3>DirectDraw Vs GDI</h3>

<p id="paperbox">
	The kind of things programmers hard to deal with around 1997 is just saddening: See John Carmack's funny comment in the source code:
	<pre class="long">
	
	
	// can be negative for stupid dibs
	
	
	</pre>
	<p>If Quake2 was running fullscreen with DirectDraw it had to draw the offscreen buffer from 
	top to bottom:
	the way it would appear on the screen. But if it was running in windowed mode with Window's GDI, it had to draw the offscreen buffer in a DIB buffer.....VERTICALLY FLIPPED because
	most Graphic vendors drivers were blitting the DIB image from RAM to VRAM in flipmode (you have to wonder if the 'I' in GDI really stands for "Independent").<br/>
	<br/>
	So the offscreen framebuffer navigation had to be abstracted via a structure and values were initialized 
	differently to abstract	the difference. This is a crude way to do polymorphism in C ;).
	<pre class="long">
	
		
    typedef struct
    {
	    pixel_t        *buffer;         // The offscreen framebuffer
	    pixel_t        *colormap;       // The color gradient palette: 256 * VID_GRADES size
	    pixel_t        *alphamap;       // The translucency palette  : 256 * 256 translucency map
	    int            rowbytes;        // may be > width if displayed in a window
                                        // can be negative for stupid dibs
	    int            width;          
	    int            height;
    } viddef_t;
	
    viddef_t vid ;
	
    
    
	</pre>
	
	<p>
		Depending on the way the framebuffer had to be drawn, <code>vid.buffer</code> was initialized to either the first pixel of:
		<ul>
			<li>First line for DirectDraw</li>
			<li>Last line for WinGDI/DBI Surface</li>
		</ul>
		<p>
		In order to navigate up or down, <code>vid.rowbytes</code> was either initialized to <code>vid.width</code> or <code>-vid.width</code>.
		<br/>
	<img style="display:inline;" src="../fd_proxy/quake2/buffer_GDI.jpeg"/>
	<img style="display:inline;" src="../fd_proxy/quake2/buffer_directdraw.jpeg"/>

	<br/>
	<br/>
	<p>
		Now to navigate whether we had to draw normally or vertically flipped doesn't matter:	
	<pre class="long">
	
    
    // First pixel of first row
    byte* pixel = vid.buffer + vid.rowbytes * 0;
    
    // First pixel of last row
    pixel = vid.buffer + vid.rowbytes * (vid.height-1);
    
    // First pixel of row i (row numbering starts at 0)
    pixel = vid.buffer + vid.rowbytes * i;
    
 	
    // Clear the framebuffer to black
    memset(vid.buffer,0,vid.height*vid.height) ;   // <-- This will work in fullScreen DirectDraw mode 
                                                   //     but crash in windowed mode :( !
	</pre>
	<br/>
	<p>
	This trick allowed the entire rendition pipeline to never worry about the underlying blitter and I
	think it is pretty awesome.
	</p>
</p>













<h3>Map preprocessing</h3>

<p id="paperbox">
Before digging in the code further it is important to understand two important databases generated during map preprocessing:<br/>
<ul>
	<li>Binary Space Partioning/ Potentialy Visible Set.</li>
	<li>Radiosity based lightmap texture.</li>
</ul>
</p>









<h3>BSP Slicing, PVS Generation</h3>

<p id="paperbox">
I invite you to read more about Binary Space Partitioning:
<ul>
	<li><a href="http://en.wikipedia.org/wiki/Binary_space_partitioning">Wikipedia</a>.</li>
	<li><a href="http://downloads.gamedev.net/pdf/gpbb/gpbb59.pdf">The Idea of BSP Trees</a> by Michael Abrash.</li>	
	<li><a href="http://downloads.gamedev.net/pdf/gpbb/gpbb60.pdf">Compiling BSP Trees</a> by Michael Abrash.</li>	
	<li>The <a href="http://www.gamers.org/dhs/helpdocs/bsp_faq.html">old bible</a> ASCII style article.</li>	
</ul>
<p>

Just like in Quake1, Quake2 maps are heavily pre-processed : The volume is sliced recursively as in the following drawing:<br/> 
				<br/> 
				 <img style="display: block; margin-left: auto; margin-right: auto;" src="../quakeSource/bsp.jpg"   /> 
				<br/> 
				<br/> 

In the end, the map had been sliced in convex 3D spaces (called clusters) and just like in Doom and Quake1 it can be used to sort all polygons around Near to Far or Far to Near.<br/>
<br/>
The great addition is the PVS (Potentially Visible Set), a set of bit vector (one entry per cluster) that can be seen as a database that can be queried to retrieve what clusters are Potentially Visible from any cluster.
This database is huge (5MB) but efficiently compressed to a few hundred of KB so it can fit nicely in RAM.<br/>
<br/>
<u><b>Note :</b></u> The Run-length compression used to shrink the PVS can skip only 0x00s, this process is detailed later. <br/> 
			</p>
</p>



















<h3>Radiosity</h3>

<p id="paperbox">
Just like in Quake1 the map's light impact were precalculated and stored in textures called lightmaps . Contrary to Quake1, Quake2 used radiosity and colored light during the precalculation. The lightmaps were then stored in the <code>PAK</code> archive and used at runtime:<br/>
	<br/>
	A few words from one of the creator: Michael Abrash in "Black Book of Programming" (Chapitre: "Quake: a post-mortem and a glimpse into the
	 future"): 
	
	<blockquote class="style1">
<br/>
<span>
<br/>
	The most interesting graphics change is in the preprocessing, where John has added support for radiosity lighting...
<br/>
<br/>
<br/>
</span>
</blockquote>	
<br/>
	<blockquote class="style1">
<br/>
<span>
<br/>
	Quake 2 levels are taking up to an hour to process. <br/> (Note, however, that that includes BSPing, PVS calculations, and radiosity lighting, which I'll discuss later.)
<br/>
<br/>
<br/>
</span>
</blockquote>	
<br/>	
	<p>
	If you don't know what radiosity illumination is, read this extremely well illustrated 
	<a href="http://freespace.virgin.net/hugo.elias/radiosity/radiosity.htm">article</a> (<a href="radiosity.zip">mirror</a>)
	: it is pure gold .<br/>
	<br/>
	Here is the first level, textured with the radiosity texture only: Unfortunately the gorgeous colored RGB had to be resampled to grayscale for the software renderer (more about this later).<br/>
	
	<br/>
	<br/>
		<img src="3colors_01.jpg"/>
	<br/>	<br/>
	The low resolution of the lightmap is striking here but since they are bilinear filtered (yes even in 
	the software renderer) the end result combined to a color texture is very good.<br/>
	<br/>
	<u>Trivia :</u>The lightmaps could be any size from 2x2 up to 17x17 (contrary to the stated 
	maxsize of 16 in flipcode's <a href="http://www.flipcode.com/archives/Quake_2_BSP_File_Format.shtml">article
	</a>(<a href="flipcode.zip">mirror</a>)) and did not have to be square.
</p>













<h3>Code architecture</h3>
<p id="paperbox">
	Most of the software renderer code is in the method <code>R_RenderFrame</code>. Here is a summary but for a more complete breakdown
	check out my <a href="softwareRenderer.notes.php">raw notes</a>. <br/>
	<br/>
	<ul>
		<li><code>R_SetupFrame</code> : Retrieve the current viewing location by walking the BSP (call to <code>Mod_PointInLeaf</code>), store the viewpoint clusterID in <code>r_viewcluster</code>.</li>
		<li><code>R_MarkLeaves</code> : Using the current view cluster (<code>r_viewcluster</code>), retrieve and decompress the PVS. Use the PVS to mark all faces visible.</li>
		<li><code>R_PushDlights</code>: Use the BSP again walking faces near to far and if the face is marked as visible, check if it is affected by a light.</li>
		<li><code>R_EdgeDrawing</code>: Map Rendition </li>
			<ul>
				<li><code>R_RenderWorld</code> : Walk the BSP Near to Far
					<ul>
						<li>Project every visible polygons in screen space: Build a Global Edge Table.</li>
						<li>Also Push a surface proxy for each visible surface on the surface stack (This is done for coherence purpose).</li>
					</ul>
				</li> 
				<li><code>R_ScanEdges</code> : Render the map scanline per scanline from top to bottom. For each line:
					<ul>
						<li><code>R_InsertNewEdges</code> : Use the Global Edge Table to initalized the Active Edge Table, use it as a set of event.</li>
						<li><code>(*pdrawfunc)()</code>  : Generate spans but don't write to offscreen buffer, instead insert the spans in a span stack buffer. Reason is detailled later.</li>
						<li><code>D_DrawSurfaces</code> : If the span stack buffer is full, render all spans to the offscreen buffer and reset the span stack.</li>
					</ul>
				
			</ul>
		<li><code>R_DrawEntitiesOnList</code> : Render entities that are not BModel (A bmodel is an element of the map). Render Sprites and AliasModels (players,enemies, ammos)...</li>
		<li><code>R_DrawAlphaSurfaces</code> :  Use a cool trick to perform per pixel blending</li>
		<li><code>R_CalcPalette</code> : Perform post effect such as color blending (damage, item pick-up)</li>			
	</ul>
	<br/>
	<pre class="long">
		
	
    R_RenderFrame
    {
	    
       R_SetupFrame            // Set pdrawfunc function pointer, depending if we are rendering for GDI or DirectDraw
       {
          Mod_PointInLeaf      // Determine what is the current view cluster (walking the BSP tree) and store it in r_viewcluster
       }
       
       
       R_MarkLeaves            // Using the current view cluster (r_viewcluster), retrieve and decompress 
                               // the PVS (Potentially Visible Set)
       
       R_PushDlights           // For each dlight_t* passed via r_newrefdef.dlights, mark polygons affected by a light.
	
       
                               // Build the Global Edge Table and render it via the Active Edge Table 
        
       R_EdgeDrawing           // Render the map	
       {	    
          R_BeginEdgeFrame     // Set function pointer pdrawfunc used later in this function 
          R_RenderWorld        // Build the Global Edget Table 
                               // Also populate the surface stack and count # surfaces to render (surf_max is the max)
          R_DrawBEntitiesOnList// I seriously have no idea what this thing is doing.
          
          
                   
          
          R_ScanEdges          // Use the Global Edge Table to maintin the Active Edge Table: Draw the world as scanlines  
                               // Write the Z-Buffer (but no read)                              
          {			
              for (iv=r_refdef.vrect.y ; iv&lt;bottom ; iv++)
              {
                R_InsertNewEdges     // Update AET with GET event
                (*pdrawfunc)();      //Generate spans
                D_DrawSurfaces       //Draw stuff on screen
              }	
              
                // Flush span buffer
                R_InsertNewEdges     // Update AET with GET event
                (*pdrawfunc)();      //Generate spans
                D_DrawSurfaces       //Draw stuff on screen
           }        
       }		
       			    
                                  	
	   
       		
	
       R_DrawEntitiesOnList    // Draw enemies, barrel etc...	
                               // Use Z-Buffer in read mode only.
	     
       R_DrawParticles	       // Duh !
       R_DrawAlphaSurfaces     // Perform pixel palette blending	ia the <code>pics/colormap.pcx</code> lower part lookup table.
       R_SetLightLevel	       // Save off light value for server to look at (BIG HACK!)

       R_CalcPalette           // Modify the palette (when taking hit or pickup item) so all colors are modified     
       
    }
	
	</pre>
</p>


































<h3>R_SetupFrame: BSP Manipulation</h3>

<p id="paperbox">
 Binary Space Partition walking is done everywhere in the code and it's a powerful constant speed mechanism to sort polygons near to far or far to near. The key is to
 understand the <code>cplane_t</code> structure:<br/>
 <br/>
 <pre class="long">
 
 
    typedef struct cplane_s
    {
        vec3_t   normal;
        float    dist;    
    } cplane_t;
 
 </pre>
 
 <p>
 	In order to calculate the distance or a point from a splitting plan in a node, we only have to inject its coordinates in the plane equation:<br/>
 </p>
 
 <br/>
 <pre class="long">
 
 
    int d = DotProduct (p,plane->normal) - plane->dist;
 
       
 </pre>
 <p>
	 With the sign of <code>d</code> we know if we are in front or behind the plane and sorting can be done. It is a process that is used from Doom to Quake3.   
 </p>
</p>












   


<h3>R_MarkLeaves: Runlength PVS decompression</h3>

<p id="paperbox">
The way I understood PVS decompression in my <a href="../quakeSource/quakeSourceRendition.php">Quake 1 Engine code review</a> was plain wrong. It is not the distance between 1 bits that is encoded but the number of 0x00 bytes to write. Only 0x00 is runlength compressed in the PVS: Upon reading the compressed stream:
<ul>
	<li>if a non-zero byte is read from the compressed PVS, is is written as is in the decompressed version.</li>
	<li>If a 0x00 byte is read from the compressed PVS, the next byte indicates how many 0x00 bytes follow.</li>
</ul>
<p>
	This means the PVS is compressed by compacting subsequent 0x00 bytes. Other bytes are left uncompressed.
	<br/>
<pre class="long">


    byte *Mod_DecompressVis (byte *in, model_t *model)
    {
        static byte  decompressed[MAX_MAP_LEAFS/8];  // A leaf = 1bit so only 65536 / 8 = 8,192 bytes
                                                     // are needed to store a fully decompressed PVS.
        int   c;
        byte   *out;
        int    row;

        row = (model->vis->numclusters+7)>>3;	
        out = decompressed;

        do
        {
            if (*in)            // This is a non null byte, write it as is and continue to the next compressed byte.
            {
                *out++ = *in++;
                continue;
        }
	
        c = in[1];              // We did not "continue" so this is a null byte: Read the next byte (in[1]) and write as
        in += 2;                // many byte as requested in the uncompressed PVS.
        while (c)
        {
            *out++ = 0;
            c--;
        }


        } while (out - decompressed < row);
	
        return decompressed;
        }

        
</pre>
<br/>
<p>
The compression scheme allows to compress at most 255 bytes containing 0x00 (255*8 leafs), a new zero must be issued with a number after to skip an other 255 bytes if necessary. Skipping 511 bytes (511*8 leafs) hence consumes 4 bytes: 0 - 255 - 0 - 255
<br/>
<br/>
Example:
<pre class="long">


    // Example of a system with 80 leafs, represented on 10 bytes: visible=1, invisible=0

    Binary      Uncompressed PVS
  
      0000 0000    0000 0000  0000 0000   0000 0000    0011 1100     1011 1111    0000 0000    0000 0000    0000 0000   0000 0000 
  
    Hexadecimal Uncompressed PVS
    
      0x00 0x00 0x00 0x00 0x39 0xBF 0x00 0x00 0x00 0x00
  
      
    // !! COMPRESSION !!  
      
      
    Binary      Compressed PVS
  
      0000 0000    0000 1000  0011 1100     1011 1111    0000 0000    0000 1000
      
    Hexadecimal Compressed PVS
    
      0x00 0x04 0x39 0xBF 0x00 0x04     
                 

</pre>
<br/>
<p>
Keep in mind that contrary to Quake 1, the PVS in Quake 2 contains not leaf IDs but cluster IDs. In order to reduce compilation time, designers were allowed to mark some walls are "detail" which were not accounted as blocking the view. As a result, the PVS contains clusterID and some leave share the same cluster ID.
<br/>
<br/>
Once the PVS of the current cluster has been decompressed, every single face belonging to a cluster designated as visible by the PVS is marked as visible too:<br/>
<br/>
<u>Q :</u> With a decompressed PVS, given a cluster ID <code>i</code>, how you test if it is visible ?<br/>
<u>A :</u> Bit AND between the i/8's byte of the PVS and 1 << (i % 8)
<pre class="long">
  
    char isVisible(byte* pvs, int i)
    {
		
	    // i>>3 = i/8    
	    // i&7  = i%8
	    
	    return pvs[i>>3] & (1<<(i&7))
	    
    } 
    
</pre>
<br/>
<p>
Just like in Quake1, there is a nice trick used to mark a polygon as "visible": Instead of using
a flag and have to reset all of them at the beginning of each frame, an <code>int</code> was used. Every beginning of frame
a framecounter <code>r_visframecount</code> was increased by one in <code>R_MarkLeaves</code>. After decompressing the PVS, all zones
are marked visible by setting its <code>visframe</code> field to the current value of <code>r_visframecount</code>.<br/>
Later in the code, node/cluster visibility is always tested as follow:
</p>
<pre class="long">


    if (node->visframe == r_visframecount)
	{
       // Node is visible
	}
	
	
</pre>

</p>
















<h3>R_PushDlights: Dynamic lighting</h3>

<p id="paperbox">
For each active dynamic light lightID the BSP walked recursively starting from the light's position. Calculate the distance
light <-> Cluster and if the light intensity if greater than the distance from the cluster, mark all surfaces in the
cluster as affected by this light ID.<br/>
<br/>
<u><b>Note :</b></u> Surfaces are marked via two fields: 
<ul>
	<li><code>dlightframe</code> is a flag <code>int</code> working in the same way clusters are marked for visibily (instead
of reseting all flags, a global variable <code>r_dlightframecount</code> is increased each frame. In order to be affected
by a light: <code>r_dlightframecount</code> must be equal to <code>surf.dlightframe</code>.</li>
<li><code>dlightbits</code> is an <code>int</code> bitvector used to store the different light's array index affecting this face.</li>
<br/>
Once the flag is raised the lights affecting the surface are encoded in an <code>int</code>, used as a bitvector. Hence at maximum 32 lights can affect a surface.
</ul>
</p>








<h3>R_EdgeDrawing: Map rendition</h3>

<p id="paperbox">
<code>R_EdgeDrawing</code> is the kraken of the software renderer, the most complex and hardest to understand. The key to
get it is to see the main data structure: 

A stack of <code>surf_t</code> (acting as proxy to <code>m_surface_t</code>) allocated on the CPU stack.

<br/>
<img src="../fd_proxy/quake2/surf_stack.png" style="float:right; margin: 95px 10px 10px 0px;"/>
<br/>


<pre class="long" style="width : 65% ;">

               

                                                                                                
    //Surface stack system                                                                     
    surf_t  *surfaces  ;   // Surface stack array                                              
    surf_t  *surface_p ;   // Surface stack top                                                
    surf_t  *surf_max  ;   // Surface stack upper limit     
                                       
    // surfaces are generated in back to front order by the bsp, so if a surf                  
    // pointer is greater than another one, it should be drawn in front                        
    // surfaces[1] is the background, and is used as the active surface stack                  

    
    
    Note: The first element in surfaces is the dummy surface
    Note: The second element in surfaces is the background surface
    
    
    
    
</pre>

<p>
This stack is populated when walking the BSP near to far. Each visible polygon is inserted in the stack as a surface
proxy. Later when walking an Active Edge Table to generate a line of the screen, it allows to see very fast which 
polygon is in front of all the others by simply comparing the address in memory (the lower in the stack = the closer
to the Point of View). This is how "coherence" is done for the scan-line conversion algorithm.<br/>
<br/>
<u>Note :</u> Each entry in the stack also features a linked list (called Texture Chain), pointing to elements in the span buffer stack (detailed just after).
Spans are stored in a buffer and drawn from the texture chain in order to group span per texture and maximize the CPU pre-caching buffer.<br/>
<br/>
The stack is initialized at the very beginning of <code>R_EdgeDrawing</code>:
</p>

<pre class="long">

    void R_EdgeDrawing (void)
    {
        // Total size: 64KB
        surf_t	lsurfs[NUMSTACKSURFACES +((CACHE_SIZE - 1) / sizeof(surf_t)) + 1];

        surfaces =  (surf_t *) (((long)&lsurfs[0] + CACHE_SIZE - 1) & ~(CACHE_SIZE - 1));
        surf_max = &surfaces[r_cnumsurfs];

        // surface 0 doesn't really exist; it's just a dummy because index 0
        // is used to indicate no edge attached to surface
        surfaces--;
        R_SurfacePatch ();
       

        R_BeginEdgeFrame ();    // surface_p = &surfaces[2];	
                                // background is surface 1,
                                // surface 0 is a dummy
	
        R_RenderWorld 
        {
	    	R_RenderFace
    	}
    	
        R_DrawBEntitiesOnList
        
        R_ScanEdges          // Write the Z-Buffer (but no read)                              
        {			
              for (iv=r_refdef.vrect.y ; iv&lt;bottom ; iv++)
              {
                R_InsertNewEdges     // Update AET with GET event
                (*pdrawfunc)();      //Generate spans
                D_DrawSurfaces       //Draw stuff on screen
              }	
              
                                     // Flush span buffer
                R_InsertNewEdges     // Update AET with GET event
                (*pdrawfunc)();      //Generate spans
                D_DrawSurfaces       //Draw stuff on screen
         }     
    }
    
        
</pre>
<p>
Here are the details:<br/>
<ul>
	<li><code>R_BeginEdgeFrame</code> : Cleanup the Global Edge Table from last frame.</li>
	<li><code>R_RenderWorld</code> : Walk the BSP (note that it does NOT render anything to the screen):
				<ul>
					<li>Every surface deemed visible is added on the stack with a <code>surf_t</code> proxy.</li>
					<li>Project all polygon's vertices in screen space and populate the Global Edge Table.</li>
					<li>Also generate the 1/Z value for each vertices so the Z-Buffer can be generated with spans.</li>
				</ul>
	</li>
	<li><code>R_DrawBEntitiesOnList</code> : I seriously have no idea what this thing is doing.</li>
	<li><code>R_ScanEdges</code> : Combine all information build so far to render the map:
		<ul>
			<img src="../fd_proxy/quake2/span_stack.png" style="float:right; margin: 82px 10px 10px 0px;"/>
			<li>Initialize the Active Edge Table.</li>
			
			<li>Initialize the span buffer (also a stack):</li>
			
			<pre class="long" style="width : 62% ;">
			
                                                                                    
    // Span stack system                                                            
    espan_t *span_p;       //Current top of the span stack pointer                  
    espan_t *max_span_p;   //Max span_p beyond this, the stack overflows            
    // Note: The span stack (basespans) allocated on the CPU stack                  
                                                                                    
              
    void R_ScanEdges (void)
    {
        int    iv, bottom;
        byte   basespans[MAXSPANS*sizeof(espan_t)+CACHE_SIZE];
              
        ...
                 
    }    
    
    
			</pre>
			<li>Start the scanline algorithm, from the top of the screen to the bottom.</li>
			<li>For each line:
				<ul>
					<li>Update the Active Edge Table with the Global Edge Table.</li>
			 		<li>Run the Active Edge Table on the entire line:</li>
			 			<ul>
			 				<li>Determine visible polygon using the address on the surface stack.</li>
			 				<li>Emit a span in the span buffer.</li>
			 				<li>If span buffer is full: Draw all the spans and reset the span stack.</li>
			 			</ul>	
				</ul>
			<li>Check if there is anything remaining in the span buffer, if yes: Draw all the spans and reset the span stack. </li>					
		</ul>
	</li>
</ul>

</p>
</p>



















<h3>R_EdgeDrawing video</h3>

<p id="paperbox">
In the following video the engine was running at 1024x768. It was also slowed down and a special cvar <code>sw_drawflat 1</code> was enabled to render polygons without textures but with different colors:<br/>
<br/>
<br/>
<br/> 
<table id="scanline_zero_overdraw"><tr><td align="center"> 

<video width="900" height="675" preload="none" poster="../fd_proxy/quake2/videos/scanLine.png" controls>
  <source src="../fd_proxy/quake2/videos/scanLinesMedium.mov" type="video/mp4">
  Your browser does not support the video tag.
</video>
</td></tr></table> 
<br/> 
<br/>
There are soooo many things to notice in this video:<br/>
<ol>
	<li>Screen is generated top to bottom this is a typical scanline algorithm.</li>
	<li>Horizontal span of pixels are not written as generated. This is done to optimized the Pentium pre-fetching cache: Spans are grouped by textureId with a mechanism
	called "texture chains". Spans are stored in a buffer. When the buffer is full: spans are drawn with texture chains in ONE batch.</li>
	<li>The moment when the span buffer is full and a rendition batch is triggered is obvious since the polygon rendering stops around the 50th pixels line .
	<li>Spans are generated top to bottom but they are drawn bottom to top: Since spans are inserted in a texture chain at the top of the listed list, spans are drawn
	in the reverse order they were produced.</li>
	<li>The last batch is covering 40% of the screen while the first one covered 10%. It is because they are far less polygon there, the spans are not cuts and they cover much more area.</li>

	<li>OMG ZERO OVERDRAW during the solid world rendition step .</li>
</ol>
</p>












<h3>Precalculated Radiosity: Lightmaps </h3>

<p id="paperbox">

    Unfortunately the nice 24bits RGB lightmaps had to be resampled to 6bits grayscale (selecting the brightest channel between R, G and B) in order to fit the palette limitation:<br/>
	<br/>
	What is stored in the <code>PAK</code> archive (24bits):<br/>
	<br/>
		<img src="3colors_01.jpg"/>
	<br/>	<br/>
	After loading from disk, and resampling to 6bits:<br/>
	<br/>
		<img src="3colors_01.gif"/>
	<br/>
		<br/>
	Now all together: The face's texture give the color in the range [0,255]. This value indexes a color in the palette from <code>pics/colormap.pcx</code>:<br/>
	<br/>
	<img src="../fd_proxy/quake2/soft_rend_1_text.jpg"/><br/>
	<br/>
	<br/>
	The lightmaps are filtered: we end up with a value [0,63].<br/>
	<br/>
	<img width="850" src="../fd_proxy/quake2/soft_rend_0_lightmap.jpg"/>
	<br/>
	<br/>
	Now using the upper part of <code>pics/colormap.pcx</code> the engine can select the right palette entry: Using the texture entry value as X coordinate and the luminance*63 as Y coordinate for the final result:<br/>	<br/>
		<Br/>
	<img src="../fd_proxy/quake2/COLORMAP_upper.jpg"/>
	<br/>
	<br/>
	Et voila:<br/>
	<br/>

	<img src="../fd_proxy/quake2/soft_rend_2_surface.jpg"/><br/>
	<br/>
	<br/>	
	I personally think it is awesome to manage to fake 64 gradients of 256 colors....will only 256 colors :) !
</p>












<h3>Surface subsystem</h3>

<p id="paperbox">
According to the previous screenshots it is obvious that surface generation is the most CPU intensive part of Quake2 (this is confirmed by the profiler results 
seen later in this page). Two mechanisms make surface generation affordable in terms of speed and memory consumption:

<ul>
	<li>Mipmapping</li>
	<li>Caching</li>
</ul>
</p>


















<h3>Surface subsystem: Mipmapping</h3>

<p id="paperbox">
When a polygon is projected in screen space, its distance 1/Z is generated. The closest vertex is used to see which mipmap level should be used.
Here is an example of a lightmap and how it is filtered according to the mipmap level:
<br/>
<br/>
<br/>
<img style="margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/surfaces/lightmap.jpg"/>
<img style="margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/surfaces/mipmap.gif"/><br/>
<br/>
<br/>

Here is a mini project I worked on in order to test Quake2's bilinear filtering quality on random images: <a href="BilinearFiltering.zip"><img src="../shmup_source_code/zip.png"/></a>
<br/>
<br/>
Following are the result of a test done on a 13x15 texels lightmap:<br/>
<br/>	
<br/>
<table style="width:100%; text-align:center;">
	<tr>
		<td><img  src="../fd_proxy/quake2/surfaces/q2.test.surface.mipmap3.jpg"/></td>
		<td><img  src="../fd_proxy/quake2/surfaces/q2.test.surface.mipmap2.jpg"/></td>
	</tr>

	<tr>
		<td>Mimpap level 3: Block are 2x2 texels.</td>
		<td>Mimpap level 2: Block are 4x4 texels.</td>
	</tr>

</table>
<br/>	
<br/>
<br/>	
<br/>
<table style="width:100%; text-align:center;">
	<tr>
		<td><img  src="../fd_proxy/quake2/surfaces/q2.test.surface.mipmap1.jpg"/></td>
		<td><img  src="../fd_proxy/quake2/surfaces/q2.test.surface.mipmap0.jpg"/></td>
	</tr>

	<tr>
		<td>Mimpap level 1: Block are 8x8 texels.</td>
		<td>Mimpap level 0: Block are 16x16 texels.</td>
	</tr>

</table>	
<br/>
<br/>
The key to understand the filtering is that everything is based on the polygons world space dimension ( width and height are called <code>extent</code>):
<ul>
 <li>The Quake2 preprocessor made sure a polygon dimensions (X or Y) were less or equal to 256 and also a multiple of 16.</li>
 <li>From a world space dimension of a polygon we can deduce:
 	  <ul>
       <li>The lightmap dimension (in texels): LmpDim = PolyDim / 16 + 1
<li>The Surface dimension (in blocks): SurDim = LmpDim -1 = PolyDim / 16
</ul>     
</ul>
<p>
In the following drawing the polygon dimension are (3,4), the lightmap is (4,5) texels and the surface degenerated will ALWAYS be (3,4) blocks. The mipmap levels decides the texels dimensions of a block and hence the overall size of the surface in texels.
<br/>

<br/><br/>
<br/><br/>
<img src="../fd_proxy/quake2/surface_gen.jpg"/>
<br/><br/>
All this is done in <code>R_DrawSurface</code>. The mipmap level is selected thanks to an array of function pointer (<code>surfmiptable</code>) selecting the right rasterizing function:<br/>
<br/>
<pre class="long">


    static void	(*surfmiptable[4])(void) = {
	R_DrawSurfaceBlock8_mip0,
	R_DrawSurfaceBlock8_mip1,
	R_DrawSurfaceBlock8_mip2,
	R_DrawSurfaceBlock8_mip3
    };


     R_DrawSurface
    {

   	pblockdrawer = surfmiptable[r_drawsurf.surfmip];					        
	for (u=0 ; u&lt;r_numhblocks; u++)						        
		(*pblockdrawer)();

    }

  
</pre>


<p>
In the following altered engine you can see three levels of mipmap: 0 in grey, 1 in yellow and 2 in red:<br/>
<br/>    
<img style="display: block; margin-left: auto; margin-right: auto;" src="soft_renderer_mipmapping.jpg"/><br/>
<br/>
<br/>
Filtering is done by block, when generating the surface's block[i][j], the filterer would use the lightmap values: lightmap[i][j],lightmap[i+1][j],lightmap[i][j+1] and lightmap[i+1][j+1]: 
essentially using the four texels located directly at the coordinate and the three ones below and to the right. Note that is is not doing weighted interpolation but rather linear interpolation 
vertically first and the interpolating horizontally again on the values generated. In short, it is working exactly as the <a href="http://en.wikipedia.org/wiki/Bilinear_filtering">wikipedia article</a> about bilinear filtering is describing it.<br/>
<br/> 
<u>All together now :</u><br/>
<br/>


<table style="width:100%; text-align:center;">
	<tr>
		<td><img  src="../fd_proxy/quake2/surfaces/lightmap_inverted.jpg"/></td>
		<td><img  src="../fd_proxy/quake2/surfaces/q2.test.surface.mipmap0_inverted.jpg"/></td>
	</tr>

	<tr>
		<td>Original Lightmap 13x15 texels.</td>
		<td>Mimpap level 0 filtered (16x16 blocks)=192x224 texels.</td>
	</tr>

</table

Result:<br/>
<br/>
<img style="margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/surfaces/quake00_outligned.jpg"/>
</p>

























<h3>Surface subsystem: Caching</h3>

<p id="paperbox">
<br/>
Even though the engine relied massively on <code>malloc</code> and <code>free</code> for memory management it still used its own memory
manager for surface caching. A block of memory was initialized as soon as the rendition resolution was known:<br/>
<br/>
<pre class="long">


    size = SURFCACHE_SIZE_AT_320X240; 

    pix = vid.width*vid.height;		
    if (pix > 64000)				
        size += (pix-64000)*3;	

        
    size = (size + 8191) & ~8191;
        
    sc_base = (surfcache_t *)malloc(size);
	sc_rover = sc_base;
	
        
</pre>
<p>
A rover <code>sc_rover</code> was placed at the beginning on the block to keep track of what had been consumed so far. When the rover reached the end of the memory
is was wrapping around, essentially recycling old surfaces. The amount of memory reserved can be seen in this graph:<br/>
<br/>
<img  style="display: block; margin-left: auto;margin-right: auto;" src="../fd_proxy/quake2/Surface&#32;Cache.jpg"/>

<br/>
<br/>Here is the way a new chunk of memory is allocated from the block:<br/>
<br/>
<pre class="long">


    memLoc = (int)&((surfcache_t *)0)->data[size];     // Skipping size+ enough space for the memory chunk header.
    
    memLoc = (memLoc + 3) & ~3;                         /FCS: Round up to a multiple of 4
    
    sc_rover = (surfcache_t *)( (byte *)new + size);
    
    
</pre>
<br/>
<p>

	<u>Notice: </u> Fast cache alignment trick (May have to go in memory system)<br/>
	<u>Notice: </u> A header is placed on top of the requested amount of memory. Very bizarre line, using a NULL (<code>((surfcache_t *)0)</code>) pointer (but it is ok since it is not deferenced).
	<br/>
	
</p>


















<h3>Perspective Projection of the Poor ?</h3>

<p id="paperbox">
Several posts on the internet suggest that Quake2 use the "perspective projection of the poor" with a simple formula and no
homogeneous coordinates or matrices (following code from <code>R_ClipAndDrawPoly</code>):
<pre class="long">

    XscreenSpace = X / Z * XScale
    YscreenSpace = Y / Z * YScale

</pre>
<p>
Where XScale and YScale are determined by the Field Of View and the screen aspect ratio.<br/>
<br/>
This perspective projection is actually equivalent to what happens in OpenGL during the GL_PROJECTION + W divide phase:
<pre class="long">

    Perspective Projection:
    =======================
                                                 Eye coordinate vector
                                                        
                                            |      X                              
                                            |      Y
                                            |      Z
                                            |      1
     --------------------------------------------------  
            Perspective Projection Matrix   |
                                            |
       XScale       0        0        0     |     XClip
          0      YScale      0        0     |     YClip
          0         0        V1       V2    |     ZClip
          0         0        -1       0     |     WClip
          
          
          
     Clip Coordinates:    
     ================                             XClip =  X * XScale
                                                  YClip =  Y * YScale     
                                                  ZClip =  /
                                                  WClip = -Z
      
     NDC Coordinate with W Divide:
     =========
     
                      XNDC = XClip/WClip =  X * XScale / -Z
                      YNDC = YClip/WClip =  Y * YScale / -Z                
                          
                      
                      
</pre> 
<br/>
First naive proof: Compare superposed screenshots.
If we look at the code

Projection of the poor ? NOPE !!
</p>






<h3>R_DrawEntitiesOnList: Sprites and Alias Entities</h3>

<p id="paperbox">
At this point in the rendition process, the map has been rendered to screen:<br/>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/soft_rend_2_surface.jpg"><br/>
<br/>
The engine also generated a 16bits z-buffer (that was written to but never read) in the process:<br/>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/depthBeforeEntities.jpg">
<br/>
<u>Note :</u> We notice that close value are "brighter" (which is contrary of OpenGL where closer is "darker"). This is because 1/Z instead of Z is stored in the Z-buffer.<br/>
<br/>
The 16bits z-buffer is stored starting at pointer <code>d_pzbuffer</code> :<br/>
<pre class="long">

   short *d_pzbuffer;

</pre>
<p>
As stated previously 1/Z is stored using the direct application of the formula described in Michael Absrash's article "Consider the Alternatives:  Quake's Hidden-Surface Removal".
It can be found in <code>D_DrawZSpans</code>:
<pre class="long">

    zi = d_ziorigin + dv*d_zistepv + du*d_zistepu;

</pre>
<br/>
<p>
If you are interested in the mathematical proof that you can indeed interpolate 1/Z, here is paper by
Kok-Lim Low:<br/>
<br/>
<a href="Perspective-Correct&#32;Interpolation.pdf"><img style="width:70px" src="../polygon_codec/pdf-big.png" style="display: block; margin-left: auto;margin-right: auto;"/></a>
<br/>
<br/>
The z-buffer outputed during the map rendition phase is now used as input so entities can be clipped properly properly.

<br/>
A few things about animated entities (players and enemies):<br/>
<ul>
	<li>Quake1 draws only keyframes but now all vertices are LERPed (<code>R_AliasSetUpLerpData</code>) for smooth animation.</li>
	<li>Quake1 used to render distance entities as "BLOB": An inaccurate but very fast rendition method. This is now gone and Quake2 renders entities normally after BoudingBox testing
	and front facing testing with cross product.</li>
</ul>
<p>
<br/>
In terms of lighting:<br/>
</p>
<ul>
	<li>No shadow are every drawn.</li>
	<li>Polygon are gouraud shaded with a hardcoded light direction (<code>{-1, 0, 0}</code> from <code>R_AliasSetupLighting</code>).</li>
	<li>The lighting intensity is based on the surface's light intensity it is currently positioned on.</li>
</ul>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/quake2_withEntities.jpg">
</p>
























<h3>Translucency in R_DrawAlphaSurfaces</h3>

<p id="paperbox">
Translucency had to be done with palette indexes. I must have wrote this 10 times 
on this page but it only express how incredibly <b>AWESOME</b> I think it is.<br/>
<br/>
Translucent polygon are rendered as follow:<br/>
<ul>
	<li>Projection of all vertices on screen space.</li>
	<li>Left Edge, Right Edge determination. Slop based system.</li>
	<li>If the surface is not warping (animated water), rendition to cache system in RAM.</li>
	
</ul>

After this, if the surface is not 100% opaque, it has to be blended in the offscreen framebuffer:<br/>
<br/>
The trick is achieved using the second part of the <code>pics/colormap.pcx</code> image as lookup to blend
a source pixel from the surface cache with a destination pixel (offscreen framebuffer):<br/>
<br/>
The source input generates the X coordinate and the destination the Y coordinate. Resulting pixel is written to the offscreen framebuffer:<br/>
</li>
</ul>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/COLORMAP_lower.jpg"/>
<br/>
<p>
Next is an animation illustrating "Before" and "After" a per pixel palette blending:<br/>
</p>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto;" id="beforeAfterTransparency" src="quake2_software_renderer.php"/>
<script> 
var colorArray = new Array("/fd_proxy/quake2/Before_per_pixel_palette_blending.jpg","/fd_proxy/quake2/After_per_pixel_palette_blending.jpg");
var COLOR_NUM = 2;
var currentImageColor = 0;
 
function stepAhead()
{
	currentImageColor++;
	currentImageColor = currentImageColor % COLOR_NUM;
	document.getElementById("beforeAfterTransparency").src = colorArray[currentImageColor];
	setTimeout(stepAhead,2000); 
}
stepAhead();
</script> 
<br/>
</p>





























<h3>R_CalcPalette: Post effect operations and Gamma correction</h3>

<p id="paperbox">
Not only the engine was capable of "per pixel palette blending" and "palette based color gradient selection", 
it could also modify its entire palette in order to indicate damages or items pick up:<br/>
<br/>
<img src="../fd_proxy/quake2/quake_damage.jpg" /><br/>
<br/>
If any "thinker" in the game DLL on the server side wanted to have a color blended at the end of the rendition process it only had to set the value in the RGBA variable <code>float player_state_t.blend[4]</code> for any player in the game.
This value was transmitted over the network, copied in <code>refdef.blend[4]</code>'s and passed to the renderer DLL (what a transit!). Upon detection it was blended with each 256s RGB entries in the palette index. After gamma corection, the palette was then re-uploaded to the graphic card.<br/>
<br/>
<code>R_CalcPalette</code> in <code>r_main.c</code> :
<pre class="long">


    // newcolor = color * alpha + blend * (1 - alpha)
    
    alpha = r_newrefdef.blend[3];
    
    premult[0] = r_newrefdef.blend[0]*alpha*255;
    premult[1] = r_newrefdef.blend[1]*alpha*255;
    premult[2] = r_newrefdef.blend[2]*alpha*255;

    one_minus_alpha = (1.0 - alpha);

    in = (byte *)d_8to24table;
    out = palette[0];
    
    for (i=0 ; i<256 ; i++, in+=4, out+=4)
        for (j=0 ; j<3 ; j++)
            v = premult[j] + one_minus_alpha * in[j];

    
</pre>
<br/>
<p>
<u><b>Trivia :</b></u> After the palette is altered via the method previously mentionned it has to be gamma corrected (in <code>R_GammaCorrectAndSetPalette</code>):<br/>
<br/>
<img style="display: block; margin-left: auto; margin-right: auto; " src="../fd_proxy/quake2/gamma_correction.jpg" /><br/>
<br/>
Gamma correction is an expensive operation, notably involving a call to <code>pow</code> and a division....and has to be perform for each R,G and B channel of a color entry !:<br/>
<pre class="long">

	
      int newValue = 255 * pow ( (value+0.5)/255.5 , gammaFactor ) + 0.5;

      
</pre>
<p>In total it is 3 calls to <code>pow</code>, 3 divisions, 6 additions and 3 multiplication for each 256 entries in the palette index: That's A LOT.<br/>
But since the input set is limited to 8 bits per channel, the full correction can be precomputed and cached in a tiny array of 256 entries:<br/>
<br/>
<pre class="long">


    void Draw_BuildGammaTable (void)
    {
        int     i, inf;
        float   g;

        g = vid_gamma->value;

        if (g == 1.0)
        {
            for (i=0 ; i<256 ; i++)
                sw_state.gammatable[i] = i;
            return;
        }
	
        for (i=0 ; i<256 ; i++)
        {
            inf = 255 * pow ( (i+0.5)/255.5 , g ) + 0.5;
            if (inf < 0)
                inf = 0;
            if (inf > 255)
                inf = 255;
            sw_state.gammatable[i] = inf;
        }
    }

</pre>
<br/>
<p>Hence a lookup table is used to do the trick (<code>sw_state.gammatable</code>) and greatly speedup the gamma correction process.<br/>
<pre class="long">


    void R_GammaCorrectAndSetPalette( const unsigned char *palette )
    {
        int i;

        for ( i = 0; i < 256; i++ )
        {
            sw_state.currentpalette[i*4+0] = sw_state.gammatable[palette[i*4+0]];
            sw_state.currentpalette[i*4+1] = sw_state.gammatable[palette[i*4+1]];
            sw_state.currentpalette[i*4+2] = sw_state.gammatable[palette[i*4+2]];
        }

        SWimp_SetPalette( sw_state.currentpalette );
    }

    
</pre>
<p>
<br/>
<u><b>Note :</b></u> You may think that LCD flat screen don't have the same gamma issue as old CRT...except that they are usually <a href="http://developer.nvidia.com/book/export/html/181">
designed to mimic CRT behavior</a> :/ !<br/>
<br/>
</p>











<h3>Code Statistics</h3>

<p id="paperbox">

A little bit of Code analysis by Cloc in order to close with the software renderer shows a total of 14,874 lines of code for this module.
It is a little bit more than 10% of the total line count but this is not representative of the overall effort since several design were tested before settling with this one.
<pre class="long">

    $ cloc ref_soft/
          39 text files.
          38 unique files.
           4 files ignored.

    http://cloc.sourceforge.net v 1.53  T=0.5 s (68.0 files/s, 44420.0 lines/s)
    -------------------------------------------------------------------------------
    Language                     files          blank        comment           code
    -------------------------------------------------------------------------------
    C                               17           2459           2341           8976
    Assembly                         9           1020            880           3849
    C/C++ Header                     7            343            293           2047
    Teamcenter def                   1              0              0              2
    -------------------------------------------------------------------------------
    SUM:                            34           3822           3514          14874
    -------------------------------------------------------------------------------


</pre>
<p>
The assembly optimization in the nine <code>r_*.asm</code> accounts for 25% of the codebase, that a pretty impressive proportion and I think quite representative
 of the effort required by the software renderer:<br/> Most of the rasterization routine were hand optimized for x86 processor by Michael Abrash. Most of the Pentium optimization discussed in his 
"Graphics Programming Black Book" can be seen in action in those files.<br/>
<br/>
<u><b>Trivia :</b></u> Some method names are the same in the book and Quake2 code (i.e: <code>ScanEdges</code>).
</p>
</p>




<h3>Profiling</h3>

<p id="paperbox">
<img style="float:left; margin: 0px 30px 10px 10px ;" src="../fd_proxy/quake2/Visual&#32;Studio&#32;Team&#32;Profiling.jpg"/>
I tried different profilers, all of them integrated to Visual Studio 2008:
<ul>
   <li>AMD Code Analysis: </li>
   <li>Intel' VTune Amplifier XE 2011</li>
   <li>Visual Studio Team Profiler</li>
</ul>
Sticking to time sampling provided VERY different results. As an example: Vtune picked up the cost of the RAM to VRAM transfer (<code>BitBlit</code>) but the two other missed it.<br/>
Instrumentation failed on both Intel and AMD's profiler (I am not masochist enough to investigate why) but
the Profiler from VS 2008 Team edition succeeded...although I would not recommend it: The game ran at 3 frames per seconds and a 20 seconds session took 1h to analyse !<br/>   
<br/>
<br/>
<br/>
<br/>
<u>Profiling with VS 2008 Team edition :</u><br/>
<br/>
<img src="../fd_proxy/quake2/soft/soft_profiler.jpg" style="display: block; margin-left: auto;margin-right: auto;box-shadow: rgb(119, 119, 119) 3px 3px 3px;"/><br/>
<br/>
Result is self-explanatory: 
<ul>
	<li>Software rendition cost is overwhelming: 89% is spent in the DLL.
	<li>Game logic is barely noticeable : 0%.
	<li>A surprising lot of time is spent in the DirectX sound DLL. 
	<li>More time is spent in <code>libc</code> than in the quake.exe kernel.
</ul>
<p>
<br/>
<img src="../fd_proxy/quake2/soft/soft_profiler_rend_dll_module.jpg" style="display: block; margin-left: auto;margin-right: auto;box-shadow: rgb(119, 119, 119) 3px 3px 3px;"/><br/>
<br/>
Looking closer at the ref_soft.dll time consumption:
<ul>
	<li>As mentioned earlier in this chapter "Writing byte to memory cost a lot":
		<ul>
			<li>Huge cost (33%) associated to building the Z-Buffer (<code>D_DrawZSpans</code>).
			<li>Huge cost (22%) associated to writing spans to offscreen buffer  (<code>D_DrawZSpans16</code>).
			<li>Huge cost (13%) associated to generating a surface upon cache miss.
		</ul>
	<li>The ScanLine algorithm cost is obvious:
		<ul>
			<li><code>R_LeadingEdge</code>
			<li><code>R_GenerateSpans</code>
			<li><code>R_TrailingEdge</code>
		</ul>
</ul>
<p>
<br/>
<u>Profiling with Intel's VTune :</u><br/>
<br/>
<img style="box-shadow: rgb(119, 119, 119) 3px 3px 3px;display: block; margin-left: auto; margin-right: auto;" src="../fd_proxy/quake2/ref_sof_full.jpg"/>
<br/>
A few things noticable:<br/>
<br/>
<ul>
	<li>18% dedicated to a common issue with software renderers: The cost to transfer the rendered image from RAM to VRAM (<code>BitBlit</code>).</li>
	<li>34% dedicated to surface rendition and caching (<code>D_DrawSurfaces</code>).</li>
	<li>08% dedicated to lerping the vertices for players/enemies animation(<code>R_AliasPreparePoints</code>).</li>
</ul>
<br/>
And even more <a href="../fd_proxy/quake2/profiling_ref_sof.jpg">complete profiling of Quake2 ref_sof</a> with VTune.
<br/>
<br/>
<u>Profiling with AMD Code Analysis:</u><br/>
<br/>
Kernel <a href="../fd_proxy/quake2/AMD_Profiler_quake2_kernel.png">Here</a> and ref_sof <a href="../fd_proxy/quake2/AMD_Profiler_quake2_ref_sof.png">here</a>.
</p>








<h3>Texture filtering</h3>

<p id="paperbox">
I have received many questions asking for directions for improving the texture filtering (to bilinear or dithered <a href="http://www.flipcode.com/archives/Texturing_As_In_Unreal.shtml">similar to Unreal</a> (<a href="../fd_proxy/quake2/unreal_kernel_filtering.png">mirror</a>)). If you want to play with this the place to go is <code>D_DrawSpans16</code> in <code>ref_soft/r_scan.c</code>:<br/>
    <br/>
    The starting screenspace coordinate (X,Y) are in <code>pspan->u</code> and <code>pspan->v</code>, you also have the width of the span in <code>spancount</code> to you can calculate what is the target screen pixel about to be generated.<br/>
    <br/>
    As for the texture coordinate: <code>s</code> and <code>t</code> are initialized at the texture original coordinates and increased by (respectively) <code>sstep</code> and <code>tstep</code> to navigate the texture sampling.<br/>
    <br/>
    Some people such as "Szilard Biro" have had good results applying Unreal I dithering technique:<br/>
    <br/>
    
    <img id="dether" src="../fd_proxy/quake2/dither_quake02.png" style="display: block; margin-left: auto;margin-right: auto;"/><br/>
    <script>
        var dether_colorArray = new Array("/fd_proxy/quake2/dither_quake02.png",
        "/fd_proxy/quake2/dither_quake03.png");
var dether_COLOR_NUM = 2;
var dether_currentImageColor = 0;
 
function dether_stepAhead()
{
	dether_currentImageColor++;
	dether_currentImageColor = dether_currentImageColor % dether_COLOR_NUM;
	document.getElementById("dether").src = dether_colorArray[dether_currentImageColor];
	setTimeout(dether_stepAhead,1000); 
}
dether_stepAhead();
    </script>
    You cand find the source code of the dithered software renderer in my <a href="https://github.com/fabiensanglard/Quake-2">Quake2 fork on github</a>. The dithering is activated by setting the cvar sw_texfilt to 1.<br/>
    <br/>
    Original dithering from Unreal 1 software renderer:<br/>
    <br/>
    
    <img id="unreal" src="../fd_proxy/quake2/unreal1.png" style="display: block; margin-left: auto;margin-right: auto;"/><br/>
    <script>
        var unreal_colorArray = new Array("/fd_proxy/quake2/unreal1.png",
        "/fd_proxy/quake2/unreal2.png");
var unreal_COLOR_NUM = 2;
var unreal_currentImageColor = 0;
 
function unreal_stepAhead()
{
	unreal_currentImageColor++;
	unreal_currentImageColor = unreal_currentImageColor % unreal_COLOR_NUM;
	document.getElementById("unreal").src = unreal_colorArray[unreal_currentImageColor];
	setTimeout(unreal_stepAhead,1000); 
}
unreal_stepAhead();
    </script>
    <br/>
</p>


<!-- <h2>Comments</h2>
<p> -->


     <!-- <div id="disqus_thread"></div> -->
    <!-- <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'fabiensanglardswebsite'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || 
                document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> -->
    <!--<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> -->
<!--     




</p> -->

 <h2 style="padding: 0px; margin: 0px;">&nbsp;</h2>
<div style="text-align:center ;">@</div>

		</div>
</div>

	<script src="../lazy_load/jquery.min.js" type="text/javascript" charset="utf-8"></script>
 	

  	<script src="../lazy_load/jquery.lazyload.min.js?v=3" type="text/javascript" charset="utf-8"></script>
	<script type="text/javascript" charset="utf-8">
		      $(function() {
		          $("img").lazyload({
		              effect : "fadeIn"
		          });
		      });
    </script>	
	</body>

</html>

